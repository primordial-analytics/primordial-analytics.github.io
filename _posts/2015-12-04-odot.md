---
layout: post
title: "Graphs, Clustering, and Weather using Snow and Ice Removal Data"
author: Ken Cavagnolo
category : ipynotebooks
tags: [python, notebook, jupyter, plotly, tableau]
comments: true
---

The Ohio Department of Transportation (ODOT) collects a limited set of
time-series data on their operations processes (materials consumed,
labor used, equipment deployed) during the snow and ice season
(approximately Nov 1 to Apr 1). I want to look explore this dataset
and see what's interesting. For simplicity, material/equipment/labor
is abbreviated (MEL).

# MEL Data

## ETL

**In [2]:**

{% highlight python %}
# dates of interest
dstr = datetime.datetime(2014, 11, 1)
dend = datetime.datetime(2015, 4, 1)
{% endhighlight %}

**In [19]:**

{% highlight python %}
# define ODOT data location
datadir = 'odot/data/odot_fy15/'
raw_equip = datadir+'FY15_Snow_and_Ice_Equipment.xlsx'
raw_labor = datadir+'FY15_Snow_and_Ice_Labor.xlsx'
raw_mater = datadir+'FY15_Snow_and_Ice_Materials.xlsx'
raw_assoc = datadir+'FY15_Snow_and_Ice_Outpost_Associations.xlsx'
{% endhighlight %}

**In [20]:**

{% highlight python %}
# read in data
equip = pd.ExcelFile(raw_equip)
labor = pd.ExcelFile(raw_labor)
mater = pd.ExcelFile(raw_mater)
assoc = pd.ExcelFile(raw_assoc)
{% endhighlight %}

**In [21]:**

{% highlight python %}
# show me how many sheets each file has in it
snames = [equip.sheet_names,
          labor.sheet_names,
          mater.sheet_names,
          assoc.sheet_names]
for n in snames:
    print n
{% endhighlight %}

    [u'FY15 Snow and Ice Equipment']
    [u'FY15 Snow and Ice Labor']
    [u'FY15 Snow and Ice Materials']
    [u'Abb Lookup']


**In [22]:**

{% highlight python %}
# keep track of all df's
dfs = []
dfs_names = []

# in luck, one sheet per df, so read only sheet in each df 
df_equip = equip.parse(equip.sheet_names[0])
dfs.append(df_equip)
dfs_names.append('equipment')

df_labor = labor.parse(labor.sheet_names[0])
dfs.append(df_labor)
dfs_names.append('labor')

df_mater = mater.parse(mater.sheet_names[0])
dfs.append(df_mater)
dfs_names.append('material')

df_assoc = assoc.parse(assoc.sheet_names[0])
dfs.append(df_assoc)
dfs_names.append('associations')

# tell me what's in each df
for i, df in enumerate(dfs):
    nulls = np.count_nonzero(df.isnull())
    print '{:15}'.format(snames[i])
    print '{:15} {:d}'.format('Observations:', df.shape[0])
    print '{:15} {:d}'.format('Features:', df.shape[1])
    print '{:15} {:d}'.format('Nulls:', nulls)
    print '\n------------------------------------------\n'
{% endhighlight %}

    [u'FY15 Snow and Ice Equipment']
    Observations:   98595
    Features:       12
    Nulls:          5001
    
    ------------------------------------------
    
    [u'FY15 Snow and Ice Labor']
    Observations:   190430
    Features:       10
    Nulls:          52
    
    ------------------------------------------
    
    [u'FY15 Snow and Ice Materials']
    Observations:   34097
    Features:       11
    Nulls:          8
    
    ------------------------------------------
    
    [u'Abb Lookup']
    Observations:   226
    Features:       3
    Nulls:          0
    
    ------------------------------------------
    


## Cleaning

Among other things, we have a NULL's problem. Let's see where they are...

**In [23]:**

{% highlight python %}
for i, df in enumerate(dfs):
    print snames[i]
    print df.isnull().sum()
    print "\n-----------------------------------------------\n"
{% endhighlight %}

    [u'FY15 Snow and Ice Equipment']
    Activity                  0
    Owner                     0
    Work Fiscal Year          0
    Work Date                 0
    Work Order #              0
    Equipment #               0
    Equipment Category        0
    Meter1 Type               0
    Meter Usage            4392
    Hours on Job              2
    Unit Cost               592
    Equipment Cost           15
    dtype: int64
    
    -----------------------------------------------
    
    [u'FY15 Snow and Ice Labor']
    Activity             0
    Owner                0
    Work Date            0
    Work Order #         0
    Employee             0
    Work Fiscal Year     0
    TRC/Paycode          0
    Total Hours          0
    Hourly Rate         26
    Labor Cost          26
    dtype: int64
    
    -----------------------------------------------
    
    [u'FY15 Snow and Ice Materials']
    Activity                     0
    Owner                        0
    Work Fiscal Year             0
    Work Date                    0
    Work Order #                 0
    Material Master Code         0
    Material Master Code Desc    0
    Amount                       0
    Unit Cost                    4
    Unit of Measure              0
    Direct Cost                  4
    dtype: int64
    
    -----------------------------------------------
    
    [u'Abb Lookup']
    District                  0
    Division / Cost Center    0
    County Abb                0
    dtype: int64
    
    -----------------------------------------------
    


**In [24]:**

{% highlight python %}
# clean-up column names and types
for df in dfs:
    df.columns = [c.lower().replace(' ', '_').replace('?', '').replace("'", "").replace("#","num") for c in df.columns]
    col_names = df.columns.tolist()
    print col_names
    print
{% endhighlight %}

    [u'activity_', u'owner', u'work_fiscal_year', u'work_date', u'work_order_num', u'equipment_num_', u'equipment_category_', u'meter1_type', u'meter_usage', u'hours_on_job', u'unit_cost', u'equipment_cost']
    
    [u'activity_', u'owner', u'work_date', u'work_order_num', u'employee', u'work_fiscal_year', u'trc/paycode_', u'total_hours', u'hourly_rate', u'labor_cost']
    
    [u'activity_', u'owner', u'work_fiscal_year', u'work_date', u'work_order_num', u'material_master_code', u'material_master_code_desc', u'amount', u'unit_cost', u'unit_of_measure', u'direct_cost']
    
    [u'district', u'division_/_cost_center', u'county_abb']
    


**In [40]:**

{% highlight python %}
df_labor["activity_"] = df_labor["activity_"].str.lower()
df_labor["act_id"], df_labor["act_name"] = zip(*df_labor["activity_"].str.split(' - ').tolist())
del(df_labor["activity_"])

df_labor["owner"] = df_labor["owner"].str.lower()
df_labor["junk"], df_labor["cost_center_name"] = zip(*df_labor["owner"].str.split(' - ').tolist())
del(df_labor["owner"])

df_labor["division_id"], df_labor["cost_center_id"] = zip(*df_labor["junk"].str.split(' ').tolist())
del(df_labor["junk"])

df_labor["employee"] = df_labor["employee"].str.lower()
salt = uuid.uuid4().hex
for e in df_labor["employee"].unique().tolist():
    hashed_emp = hashlib.sha224(e + salt).hexdigest()
    df_labor.loc[df_labor.employee == e, 'uuid'] = hashed_emp
del(df_labor["employee"])

df_labor["trc/paycode_"] = df_labor["trc/paycode_"].str.lower()
df_labor["trc"], df_labor["paycode"] = zip(*df_labor["trc/paycode_"].str.split(' - ').tolist())
del(df_labor["trc/paycode_"])
{% endhighlight %}

**In [43]:**

{% highlight python %}
field = "activity_"
splitter = ' - '
nfields = ["act_id", "act_name"]
df_equip[field] = df_equip[field].str.lower()
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])

field = "owner"
splitter = ' - '
nfields = ["junk", "cost_center_name"]
df_equip[field] = df_equip[field].str.lower()
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])

field = "junk"
splitter = ' '
nfields = ["division_id", "cost_center_id"]
df_equip[field] = df_equip[field].str.lower()
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])

field = "equipment_category_"
splitter = ' - '
nfields = ["equip_id", "equip_name"]
df_equip[field] = df_equip[field].str.lower()
df_equip[field] = df_equip[field].str.replace("&", " ")
df_equip[field] = df_equip[field].str.replace(",", " ")
df_equip[nfields[0]], df_equip[nfields[1]] = zip(*df_equip[field].str.split(splitter).tolist())
del(df_equip[field])
{% endhighlight %}

**In [46]:**

{% highlight python %}
field = "activity_"
splitter = ' - '
nfields = ["act_id", "act_name"]
df_mater[field] = df_mater[field].str.lower()
df_mater[nfields[0]], df_mater[nfields[1]] = zip(*df_mater[field].str.split(splitter).tolist())
del(df_mater[field])

field = "owner"
splitter = ' - '
nfields = ["junk", "cost_center_name"]
df_mater[field] = df_mater[field].str.lower()
df_mater[nfields[0]], df_mater[nfields[1]] = zip(*df_mater[field].str.split(splitter).tolist())
del(df_mater[field])

field = "junk"
splitter = ' '
nfields = ["division_id", "cost_center_id"]
df_mater[field] = df_mater[field].str.lower()
df_mater[nfields[0]], df_mater[nfields[1]] = zip(*df_mater[field].str.split(splitter).tolist())
del(df_mater[field])

mcode = [n[1] for n in df_mater["material_master_code"].str.split().tolist()]
df_mater["material_master_code"] = mcode

field = "material_master_code_desc"
df_mater[field] = df_mater[field].str.replace('#',"lb")
df_mater[field] = df_mater[field].str.lower()
{% endhighlight %}

**In [49]:**

{% highlight python %}
field = "division_/_cost_center"
splitter = ' - '
nfields = ["division", "cost_center"]
df_assoc[field] = df_assoc[field].str.lower()
df_assoc[nfields[0]], df_assoc[nfields[1]] = zip(*df_assoc[field].str.split(splitter).tolist())
del(df_assoc[field])

df_assoc["division_id"], df_assoc["cost_center_id"] = zip(*df_assoc["division"].str.split(" ").tolist())
del(df_assoc["division"])

df_assoc["county_abb"] = df_assoc["county_abb"].str.lower()
{% endhighlight %}

**In [51]:**

{% highlight python %}
df_assoc["division_id"].unique()
{% endhighlight %}




    array([u'0001', u'0002', u'0003', u'0004', u'0005', u'0006', u'0007',
           u'0008', u'0009', u'0010', u'0011', u'0012'], dtype=object)



**In [52]:**

{% highlight python %}
ccid = sorted(df_assoc["cost_center_id"].unique().tolist())
l = df_labor["cost_center_id"].unique().tolist()
m = df_mater["cost_center_id"].unique().tolist()
e = df_equip["cost_center_id"].unique().tolist()
a = l+m+e
for n in list(set(a)):
    if n not in ccid:
        print n, "missing from ccid "
        
did = sorted(df_assoc["division_id"].unique().tolist())
l = df_labor["division_id"].unique().tolist()
m = df_mater["division_id"].unique().tolist()
e = df_equip["division_id"].unique().tolist()
a = l+m+e
for n in list(set(a)):
    if n not in did:
        print n, "missing from did "
{% endhighlight %}

    5440 missing from ccid 
    6111 missing from ccid 
    5430 missing from ccid 
    5410 missing from ccid 
    5400 missing from ccid 
    5420 missing from ccid 


**In [53]:**

{% highlight python %}
# clean-up column names and types
for df in dfs:
    for c in df.columns.tolist():
        testval = df[c][0]
        if isinstance(testval, unicode):
            if testval.isdigit():
                print 'Fix? ', c, type(testval), " to int"
                #df[c] = df[c].astype(int)
    print
{% endhighlight %}

    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    Fix?  equip_id <type 'unicode'>  to int
    
    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    
    Fix?  material_master_code <type 'unicode'>  to int
    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    
    Fix?  division_id <type 'unicode'>  to int
    Fix?  cost_center_id <type 'unicode'>  to int
    


**In [54]:**

{% highlight python %}
# what's in the df's
for df in dfs:
    for c in sorted(df.columns.tolist()):
        print c
    print "\n-----------------------------------------------\n"
{% endhighlight %}

    act_id
    act_name
    cost_center_id
    cost_center_name
    division_id
    equip_id
    equip_name
    equipment_cost
    equipment_num_
    hours_on_job
    meter1_type
    meter_usage
    unit_cost
    work_date
    work_fiscal_year
    work_order_num
    
    -----------------------------------------------
    
    act_id
    act_name
    cost_center_id
    cost_center_name
    division_id
    hourly_rate
    labor_cost
    paycode
    total_hours
    trc
    uuid
    work_date
    work_fiscal_year
    work_order_num
    
    -----------------------------------------------
    
    act_id
    act_name
    amount
    cost_center_id
    cost_center_name
    direct_cost
    division_id
    material_master_code
    material_master_code_desc
    unit_cost
    unit_of_measure
    work_date
    work_fiscal_year
    work_order_num
    
    -----------------------------------------------
    
    cost_center
    cost_center_id
    county_abb
    district
    division_id
    
    -----------------------------------------------
    


## Postgresql and PostGIS

All clean. Can we join these dataframes in some meaningful way? Moving
the data into PSQL for posterity. In case on OSX:

{% highlight tcsh %}
brew install postgres
brew install postgis
createuser -P -s -e postgres # needed since not created by default sometimes
ln -sfv /usr/local/opt/postgresql/*.plist ~/Library/LaunchAgents # start PSQL at login
launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist # ensure empty
launchctl load ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist # load
{% endhighlight %}

Okay to get data into a DB...

{% highlight tcsh %}
createdb odot
createlang plpgsql odot
createlang plpythonu odot
psql -d odot -c "CREATE EXTENSION postgis;"
psql -d odot -c "CREATE EXTENSION postgis_topology;"
psql -d odot -c "CREATE EXTENSION postgis_sfcgal;"
psql -d odot -c "CREATE EXTENSION fuzzystrmatch;"
psql -d odot -c "CREATE EXTENSION address_standardizer;"
psql -d odot -c "CREATE EXTENSION address_standardizer_data_us;"
psql -d odot -c "CREATE EXTENSION postgis_tiger_geocoder;"
{% endhighlight %}

We'll also want some US geographies to play around with. I love the
[GADM dataset](http://www.gadm.org/) so let's use those for now. ODOT
also has an awesome GIS site [TIMS](http://gis.dot.state.oh.us/tims),
so let's grab that data too, specifically districts, facilities (for
future routing optimization), and snow/ice routes:

{% highlight tcsh %}
mkdir -p /odot/maps; cd /odot/maps
wget http://biogeo.ucdavis.edu/data/gadm2.8/shp/USA_adm_shp.zip; unzip USA_adm_shp.zip
wget http://gis.dot.state.oh.us/tims/Data/Download?output.zip; unzip output.zip
shps="USA_adm0 USA_adm1 USA_adm2 odot_districts odot_facilities snow_ice_priority_routes"
for i in $shps
do
    shp2pgsql -I -s 4326 -W "LATIN1" $i.shp $i > $i.sql
    psql -d odot -q -f $i.sql
done
{% endhighlight %}

## Load ODOT Data into PSQL

**In [55]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')

# write the dataframes to psql
for i, df in enumerate(dfs):
    df.to_sql(dfs_names[i], engine, if_exists='replace', index=False)
engine.dispose()
{% endhighlight %}

## County Aggregations

Need to get all the MEL tables to be associated with a county. First
need to have county names in the associations table. Below is the
pythonic way, then below that is straight SQL.

**In [None]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()

# while I'm at it, add county name to associations table
tname = 'some_table_name'
query = []
query.append("some sql {0} some sql;")

# run the queries
for q in query:
    conn.execute(q.format(tname))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

Add a county name to associations table:
{% highlight sql %}
ALTER TABLE associations ADD COLUMN county text DEFAULT '';
UPDATE associations AS a SET county=c.name_2 FROM usa_adm2 c WHERE c.name_1='Ohio' and substring(lower(c.name_2),1,3) = a.county_abb;
{% endhighlight %}

Some counties don't map 1:1, do by-hand:
{% highlight sql %}
UPDATE associations SET county='Ashtabula' WHERE county_abb='atb';
UPDATE associations SET county='Harrison' WHERE county_abb='has';
UPDATE associations SET county='Champaign' WHERE county_abb='chp';
UPDATE associations SET county='Monroe' WHERE county_abb='moe';
UPDATE associations SET county='Montgomery' WHERE county_abb='mot';
UPDATE associations SET county='Morgan' WHERE county_abb='mrg';
UPDATE associations SET county='Morrow' WHERE county_abb='mrw';
UPDATE associations SET county='Meigs' WHERE county_abb='meg';
{% endhighlight %}

Also need to add the county name to the MEL tables on each line
{% highlight sql %}
alter table material add column county text default '';
update material as m set county=a.county from associations a where a.cost_center = m.cost_center_name;

alter table material add column district integer default 0;
update material as m set district=a.district from associations a where a.cost_center = m.cost_center_name;

alter table equipment add column county text default '';
update equipment as e set county=a.county from associations a where a.cost_center = e.cost_center_name;

alter table equipment add column district integer default 0;
update equipment as e set district=a.district from associations a where a.cost_center = e.cost_center_name;

alter table labor add column county text default '';
update labor as l set county=a.county from associations a where a.cost_center = l.cost_center_name;

alter table labor add column district integer default 0;
update labor as l set district=a.district from associations a where a.cost_center = l.cost_center_name;
{% endhighlight %}

To get the total spend, add everything up:

{% highlight sql %}
drop table summary;
create table summary as
select initcap(m.county) as county,
       m.sum as mater_sum,
       e.sum as equip_sum,
       l.sum as labor_sum,
       m.sum + e.sum + l.sum as total_sum
from (select lower(county) as county, sum(direct_cost) from material group by 1) as m
join (select lower(county) as county, sum(equipment_cost) from equipment group by 1) as e on e.county=m.county
join (select lower(county) as county, sum(labor_cost) from labor group by 1) as l on l.county=m.county;
{% endhighlight %}

Also adding the total lane miles per county:
{% highlight sql %}
ALTER TABLE summary DROP COLUMN IF EXISTS lane_miles;
ALTER TABLE summary ADD COLUMN lane_miles numeric DEFAULT 0.0;
UPDATE summary AS s SET lane_miles=sub1.lane_mile FROM (
    select initcap(lower(county)) as county,
           sum(segment_le) as lane_mile
    from snow_ice_priority_routes
    group by 1) as sub1
    WHERE sub1.county=s.county;
{% endhighlight %}

# Weather Data

## ForecastIO Weather Data

Need a specific definition for a weather event the calls for road
pre-treatment or treatment. The closest is >50t of material used in a
24h period. How about we let the weather guide our definition? For
example:

* Rain-to-ice, sleet, or snow are expected to fall, or are falling
* In a given county, what is the relationship between material+equipment+labor (MEL) mobilization and accumulation?
* In that question are two parts:
    a. MEL consumed over a given time period per county; we have this
    b. Accumulation per county in a given time frame

To get 'b,' I'll need weather data per county per hour over the
treatment season. Can scrape this from the
[forecast.io API](https://developer.forecast.io). The API is
accessible via a nice little python library:
[python-forecastio](https://github.com/ZeevG/python-forecast.io). Let's
test it out...

**In [43]:**

{% highlight python %}
import forecastio
api_key = os.environ.get('FIO_APIKEY')
{% endhighlight %}

**In [44]:**

{% highlight python %}
# for one day only and one location
lat = 39.759087
lng = -84.189806
date = datetime.datetime(2015,2,2)
forecast = forecastio.load_forecast(api_key, lat, lng, time=date, units="us")
{% endhighlight %}

**In [51]:**

{% highlight python %}
daily = forecast.daily()
hourly = forecast.hourly()
alerts = forecast.alerts()

for d in daily.data:
    for x in d.d:
        print x, d.d[x]

print "-----------"

for a in alerts:
    for x in d.d:
        print x, d.d[x]

print "-----------"
        
for h in hourly.data:
    if 'precipType' in h.d:
        ti = str(datetime.datetime.fromtimestamp(h.d['time']))
        pt = h.d['precipType']
        print "%s: %s" % (ti, pt)
{% endhighlight %}

    apparentTemperatureMinTime 1422889200
    precipType snow
    cloudCover 0.41
    precipIntensityMaxTime 1422853200
    temperatureMin 16.32
    summary Partly cloudy in the afternoon.
    dewPoint 13.38
    apparentTemperatureMax 30.17
    temperatureMax 37.68
    temperatureMaxTime 1422853200
    windBearing 312
    moonPhase 0.46
    precipAccumulation 0.024
    visibility 7.87
    sunsetTime 1422917885
    pressure 1015.47
    precipProbability 0.12
    apparentTemperatureMin 0.54
    precipIntensityMax 0.0032
    icon partly-cloudy-day
    apparentTemperatureMaxTime 1422853200
    humidity 0.72
    windSpeed 11.44
    time 1422853200
    precipIntensity 0.0001
    sunriseTime 1422881121
    temperatureMinTime 1422932400
    -----------
    -----------
    2015-02-02 00:00:00: rain


## Weather Virtual Sampling Sites

Now the problem is sampling. Assuming that the decision to treat the road network correlates tightly with forecasted and actual weather over an area of "large enough" interest to warrant treatment, then I want to keep track of weather conditions across all counties uniformly. So two questions:
* What is "large enough?"
* Based on that value, how should I sample each county?

How about this? To keep things simple, use the average size of a county to dictate sampling. Then I can simply build a grid in lat/lng space and get weather values on each grid point across the entire state. The assumption is that this grid, on average, samples each county uniformly and thus the weather as well. To tackle the above, I loaded the GADM US admin0-2 shapes into PSQL (stored under ODOT db).
{% highlight sql %}
select min(st_area(geom)), max(st_area(geom)), avg(st_area(geom)), stddev(st_area(geom)) from usa_adm2 where name_1='Ohio';

        min        |        max        |        avg        |       stddev       
-------------------+-------------------+-------------------+--------------------
 0.064875955751619 | 0.199309554416709 | 0.128783091575315 | 0.0245318590471429

{% endhighlight %}


So unfortunately the land area of each county varies quite a lot. What about a bounding box for the extent of each county?
{% highlight sql %}
select st_extent(geom) from usa_adm2 where name_1='Ohio' and name_2='Allen';

BOX(-84.3975677490234 40.6426620483398,-83.8797378540038 40.920711517334)
{% endhighlight %}

That is indeed correct for the lat(min,max) and lng(min,max). So in a single bounding box, create a uniform sampling of AxB grid. Let's say every... I don't know. [The average thunderstorm has a diameter of 15mi](http://www.nssl.noaa.gov/primer/tstorm/tst_basics.html) (or 24.1402 km), so that sounds good. Logic block is then:

* for every county
    * get extent
    * iterate over lat from lat_min to lat_max adding steps of 25 km
        * iterate over lng from lng_min to lng_max adding steps of 25 km
            * append (x,y) to coords[]
* for every coord in coords
    * for every date
        * query forecastio
            * for every attribute in weather data
                * store into a dict
* build dataframe from dict
* send to PSQL

Major consideration here: data size. There are 88 counties in OH. The snow/ice season runs from Nov 10th-ish to Mar 10th-ish, or approx 120 days, or 2928 hours. If I keep hourly data for every county, that's 260K points. If there are 10 sampling locations per county, that's 2.6M points. The calls to the forecastio API are per day per (lat/lng), so that's 106K requests (at least). I get 1K req/day for free, so to do it for free will take 106 days. Otherwise, calls are 0.0001 each. I'd pay for 105K calls, or 10.50. Even at 10x the points, this looks reasonable (105). In the extreme, I'd use 11/1 to 4/1 (153d --> 3672h) and have avg of 20 pts/county. So, sample the space first to determine number of points in grid.

**In [52]:**

{% highlight python %}
# get the bounding box of ohio

# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()

# get ohio's extent
min_lng = [x for x in conn.execute("select st_xmin(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]
max_lng = [x for x in conn.execute("select st_xmax(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]
min_lat = [x for x in conn.execute("select st_ymin(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]
max_lat = [x for x in conn.execute("select st_ymax(st_extent(geom)) from usa_adm1 where name_1='Ohio';")][0][0]

print "Min(lat,lng): ", min_lat, min_lng
print "Max(lat,lng): ", max_lat, max_lng

# clean-up the engine
conn.close()
engine.dispose()
{% endhighlight %}

    Min(lat,lng):  38.4025001526 -84.8199310303
    Max(lat,lng):  42.9616279602 -78.853012085


Turns out the GADM shapes include waterway rights boundaries, so Ohio
technically extends all the way to Buffalo. This screws up the
bounding box method using Ohio. For now, just hardcode the upper-right
for now: 41.977296, -80.519410. Note thatt, latitude: 1 deg = 110.574
km, longitude: 1 deg = 111.320*cos(latitude) km

**In [53]:**

{% highlight python %}
# cover ohio in a grid
state = 'Ohio'
max_lng = -80.519410 # fudge it
max_lat = 41.977296  # fudge it

# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()

# iterate over lat, lng bounds
grid = []
spacer = 24.0/2.
step_lat = spacer/110.574
clat = min_lat
snum = 0
while clat <= max_lat:
    clng = min_lng
    step_lng = spacer/(111.320*cos(radians(clat)))
    while clng <= max_lng:
        query = '''
                select st_intersects(st_setsrid(st_makepoint({0}, {1}), 4326), geom)
                from usa_adm1 where name_1='{2}';
                '''
        check = [x for x in conn.execute(query.format(clng, clat, state))][0][0]
        if check:
            grid.append(('fio'+str(snum), clat, clng))
            snum += 1
        clng = clng + step_lng
    clat = clat + step_lat
    
# clean-up the engine
conn.close()
engine.dispose()

# cost for fio
gsize = len(grid) # size of grid
d0 = datetime.date(2014, 11, 1)
d1 = datetime.date(2015, 4, 1)
delta = d0 - d1
days = abs(delta.days)
dpnts = days*24*gsize
calls = days*gsize
cost = (calls-1000)*0.0001
print "Grid size:", gsize
print "API calls:", calls
print '{} {:.2f}'.format("Calls cost:", cost)
print '{} {:.4e}'.format("Data points:", dpnts)
{% endhighlight %}

    Grid size: 774
    API calls: 116874
    Calls cost: 11.59
    Data points: 2.8050e+06


**In [None]:**

{% highlight python %}
# add all the fio sampling points to psql
# make a df of the samplings
fios = pd.DataFrame(grid, columns=['sampling','lat','lng'])

# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()

# write the df to psql
tname = 'fio_sampling'
fios.to_sql(tname, engine, if_exists='replace', index=False)

# add a geom col and index to new table and 
query = ["SELECT AddGeometryColumn ('public', '{0}', 'geom', 4326, 'POINT', 2);"]
query.append("UPDATE {0} SET geom = ST_SetSRID(ST_MakePoint(lng, lat), 4326);")
query.append("CREATE INDEX {0}_geom_idx ON public.{0} USING gist(geom);")

# add the county name in which this sampling lies
query.append("ALTER TABLE fio_sampling ADD COLUMN county text DEFAULT '';")
query.append("UPDATE {0} AS s SET county=c.name_2 FROM usa_adm2 c WHERE st_intersects(c.geom, s.geom);")

# run the queries
for q in query:
    conn.execute(q.format(tname))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

What about the sampling rate per county, i.e. are we seeing too much/little data from an area?
{% highlight sql %}
select avg(srate), stddev(srate) from (select c.gid as gid, c.name_2 as name, coalesce(count(s.sampling),0)/st_area(c.geom) as srate from usa_adm2 c left join fio_sampling s on st_intersects(c.geom, s.geom) where c.name_1='Ohio' group by 1,2);
{% endhighlight %}
Avg: 64.4 +/- 11.1

Only counties outside 2-sigma are:
{% highlight sql %}
    name    |      srate       
------------+------------------
 Lake Erie  | 31.2832998026624
 Morgan     | 87.1923348516827
{% endhighlight %}

Lake Erie is an artifact of the shapefile, so this is fine for the
course sampling in lat/lng. Let's scale this up and scrape some
data...

## Download FIO Weather Data

**In [None]:**

{% highlight python %}
# timeframe
datadir = '/odot/data/fio/'

# iter over each date
tdate = dstr
while tdate <= dend:

    # status and checks
    print '{} {:%m-%d-%Y}'.format("Fetching data for:", tdate)
    ofile = datadir+'{:%Y%m%d}'.format(tdate)+".csv"
    if os.path.isfile(ofile):
        print '{}'.format("Already have data for this date: skipping.")
        tdate += datetime.timedelta(days=1)
        continue
    
    # initialize a dict of all vals in FIO
    fio_idx = []
    fio_data = {}
    attributes = ['time', 'summary', 'icon', 'sunriseTime',
                  'sunsetTime', 'moonPhase', 'nearestStormDistance',
                  'nearestStormBearing', 'precipIntensity', 'precipIntensityMax',
                  'precipIntensityMaxTime', 'precipProbability', 'precipType',
                  'precipAccumulation', 'temperature', 'apparentTemperature',               
                  'dewPoint', 'windSpeed', 'windBearing', 'cloudCover',
                  'humidity', 'pressure', 'visibility', 'ozone', 'temperatureMin',
                  'temperatureMinTime', 'temperatureMax', 'temperatureMaxTime',
                  'apparentTemperatureMin', 'apparentTemperatureMinTime',
                  'apparentTemperatureMax', 'apparentTemperatureMaxTime']
    for attr in attributes:
        fio_data[attr] = []
    
    # iter over the grid
    for g in tqdm(grid):
    
        # set some params
        fio = g[0]
        lat = g[1]
        lng = g[2]
        
        # call the FIO api
        forecast = forecastio.load_forecast(api_key, lat, lng, time=tdate, units="us")
        h = forecast.hourly()
        d = h.data
        for p in d:
            fio_idx.append(fio)
            for attr in attributes:
                if attr in p.d:
                    fio_data[attr].append(p.d[attr])
                else:
                    fio_data[attr].append(0.0)
           
    # save to df and then to csv, just in case
    df_fio = pd.DataFrame(fio_data, index=fio_idx)
    df_fio.to_csv(ofile, encoding='utf-8')
    
    # increment date
    tdate += datetime.timedelta(days=1)
{% endhighlight %}

Paranoia meant I saved everything above to CSV's, and now need those
in PSQL. Below checks file sizes and moves them into PSQL.

**In [None]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')

# table to build
datadir = '/odot/data/fio/'
tname = 'fio_weather'

# get all files
import glob
files = sorted(glob.glob(datadir+'*.csv'))
n = 0
for f in files:
    nlines = sum(1 for line in open(f))
    nlines -= 1
    if nlines != gsize*24:
        print '{} {} {}'.format("File incorrect size:", f, nlines)
        continue

    # read files
    df = pd.read_csv(f)
    df.rename(columns={'Unnamed: 0': 'sampling'}, inplace=True)
    df.columns = map(str.lower, df.columns)
    if n < 1:
        action = 'replace'
    else:
        action = 'append'
    print '{} {} {}'.format(action, f, "to"+tname)
    df.to_sql(tname, engine, if_exists=action, index=False)
    n += 1

# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

The 20141102.csv data has no dupes, so import it anyways.

**In [31]:**

{% highlight python %}
df = pd.read_csv('/odot/data/fio/20141102.csv')
df.rename(columns={'Unnamed: 0': 'sampling'}, inplace=True)
df.columns = map(str.lower, df.columns)
df.to_sql('fio_weather', engine, if_exists='append', index=False)
{% endhighlight %}

Okay, now in PSQL I have:

* fio_sampling -- sampling sites across Ohio
* fio_weather -- weather from 11/1 to 4/1 for all sites
* odot_districts -- service districts
* snow_ice_priority_routes -- plow routes
* usa_adm2 -- Ohio counties
* MEL -- material, equip, labor for ODOT in 2014-2015 snow/ice season

Let's also add the geom to each sampling and include the county name:
{% highlight sql %}
SELECT AddGeometryColumn ('public', 'fio_weather', 'geom', 4326, 'POINT', 2);
UPDATE fio_weather AS w SET geom=s.geom FROM fio_sampling s WHERE s.sampling=w.sampling;
CREATE INDEX fio_weather_geom_idx ON public.fio_weather USING gist(geom);

ALTER TABLE fio_weather ADD COLUMN county text DEFAULT '';
UPDATE fio_weather AS w SET county=c.name_2 FROM usa_adm2 c WHERE st_intersects(c.geom, w.geom);

{% endhighlight %}

## FIO Weather Data Aggregation

ODOT has their own definition based on tonage use, but what does
material use look like as a function of weather conditions? Let's try
plotting material use against precip accumulation, temp, etc. How
about a roll-up of snowfall per county per day? I have weather per
hour per day per sample site. So what I want is to find the average
precip over a region per hour, then sum those averages in a day per
county...

{% highlight sql %}
drop table fio_weather_agg;
create table fio_weather_agg as
select sub.date as date,
       sub.county,
       round(sum(sub.snow_acc)::numeric,3) as tot_snow,
       round(sum(sub.rain_acc)::numeric,3) as tot_rain,
       round(avg(sub.at)::numeric, 1) as app_temp,
       round(avg(sub.t)::numeric, 1) as air_temp,
       round(avg(sub.cc)::numeric, 2) as cloudcover,
       round(avg(sub.d)::numeric, 1) as dewpoint,
       round(avg(sub.h)::numeric, 2) as humidity,
       round(avg(sub.o)::numeric, 3) as ozone,
       round(avg(sub.p)::numeric, 1) as pressure,
       round(avg(sub.v)::numeric, 1) as visibility,
       round(avg(sub.wb)::numeric, 0) as wind_bearing,
       round(avg(sub.ws)::numeric, 1) as wind_speed
from (
      select to_timestamp(time)::date as date,
             date_part('hour', to_timestamp(time)) as hour,
             county,
             avg(apparenttemperature) as at,
             avg(temperature) as t,
             avg(cloudcover) as cc,
             avg(dewpoint) as d,
             avg(humidity) as h,
             avg(ozone) as o,
             max(precipaccumulation) as snow_acc,
             max(precipintensity) as rain_acc,
             avg(pressure) as p,
             avg(visibility) as v,
             avg(windbearing) as wb,
             avg(windspeed) as ws
      from fio_weather
      group by 1,2,3) as sub
group by 1,2
order by 1,2 asc;
{% endhighlight %}

Gut check: The snowfall totals are inconsistent with historicals from
various sites (NOAA, Weatherunderground, etc.). The FIO docs say that
historical data will be back-filled if available, but otherwise they
fall back to predicted values. **No flag is provided to indicate is
it's recorded or forecasted data!** This means the data is a mix of
measured and predicted. Not good for modeling. Let's get the daily
NOAA data for Ohio between 11/1 and 4/1. First add the FIO data as a
forecasted value (for_snow) in the PSQL db's.

**In [46]:**

{% highlight python %}
# Let's add the snowfall totals to the summary table:
# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()
query = '''
        ALTER TABLE summary DROP COLUMN IF EXISTS {0};
        ALTER TABLE summary ADD COLUMN {0} numeric DEFAULT 0.0;
        UPDATE summary AS s SET {0}=sub2.{0} FROM (
            select round(sum(sub1.accumulation)::numeric,4) as for_snow,
                   sub1.county
            from (
                  select max(precipaccumulation) as accumulation,
                         to_timestamp(time)::date as date,
                         date_part('hour', to_timestamp(time)) as hour,
                         county
                  from fio_weather
                  where preciptype='snow'
                  group by 2,3,4
                  order by 2,3 asc) as sub1
            group by 2) as sub2
        WHERE sub2.county=s.county;'''

fields = ['for_snow']
for f in fields:
    conn.execute(query.format(f))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

## NOAA Weather Data

NOAA data for all stations can be collected from this site:
[CDO Data Tools](http://www.ncdc.noaa.gov/cdo-web/datatools/selectlocation). Compared
to data in FIO, I want to focus on:

* [x] tavg: avg temp == air_temp (dummy as 0.5(min+max))
* [✔] tmax: max temp == no equiv
* [✔] tmin: min temp == no equiv
* [x] acmc: cloud cover == cloudcover
* [x] awdr: wind bearing == wind_bearing
* [✔] awnd: wind speed == wind_speed
* [x] frgb: frozen ground layer base == no equiv
* [x] frgt: frozen ground layer top == no equiv
* [x] frth: frozen ground layer thickness == no equiv
* [✔] psun: % daily of possible sunshine == no equiv
* [✔] tsun: daily sunshine == no equiv
* [✔] snow: snowfall == tot_snow
* [✔] prcp: precipitation == tot_rain
    
no app_temp, dewpoint, humidity, ozone, pressure, visibility, 

**In [5]:**

{% highlight python %}
# read csv and fix
datadir = '/odot/data/noaa/'
df = pd.read_csv(datadir+'noaa_OH_20140701-20150630_weather.csv')
df.columns = map(str.lower, df.columns)
df.columns = [c.replace(' ', '_').replace('.','') for c in df.columns]
df['date'] = pd.to_datetime(df['date'].astype(str), format='%Y%m%d')
df = df.replace(-9999.0, np.nan)
df['snow'] = df['snow']*0.0393701 # convert from mm to inches
df['prcp'] = df['prcp']/10.*0.0393701 # convert from tenths of mm to inches

# tell me what's in each df
nulls = np.count_nonzero(df.isnull())
print '{:15} {:d}'.format('Observations:', df.shape[0])
print '{:15} {:d}'.format('Features:', df.shape[1])
print '{:15} {:d}'.format('Nulls:', nulls)
if nulls > 0:
    print "\n---------------NULL's--------------------------\n"
    for c in df.columns:
        ns = df[c].isnull().sum()
        if ns > 0.9*df.shape[0]:
            print '{} {}'.format("Removing column", c)
            del(df[c])
    print "\n-----------------------------------------------\n"
{% endhighlight %}

    Observations:   107757
    Features:       41
    Nulls:          3404440
    
    ---------------NULL's--------------------------
    
    Removing column mxpn
    Removing column mnpn
    Removing column evap
    Removing column sx32
    Removing column sx52
    Removing column sn32
    Removing column sn52
    Removing column mdpr
    Removing column dapr
    Removing column psun
    Removing column tsun
    Removing column wesd
    Removing column wesf
    Removing column wdmv
    Removing column awnd
    Removing column wdf2
    Removing column wdf5
    Removing column wsf2
    Removing column wsf5
    Removing column pgtm
    Removing column wt09
    Removing column wt01
    Removing column wt06
    Removing column wt05
    Removing column wt02
    Removing column wt11
    Removing column wt04
    Removing column wt08
    Removing column wt03
    
    -----------------------------------------------
    


Drop this data into the PSQL db for posterity and analysis.

**In [6]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()
tname = 'noaa_weather'

# put into table
df.to_sql(tname, engine, if_exists='replace', index=False)

# give table geo capabilities
query = ["SELECT AddGeometryColumn ('public', '{0}', 'geom', 4326, 'POINT', 2);"]
query.append("alter table noaa_weather rename latitude to lat;")
query.append("alter table noaa_weather rename longitude to lng;")
query.append("UPDATE {0} SET geom = ST_SetSRID(ST_MakePoint(lng, lat), 4326);")
query.append("CREATE INDEX {0}_geom_idx ON public.{0} USING gist(geom);")
query.append("ALTER TABLE {0} ADD COLUMN county text DEFAULT '';")
query.append("UPDATE {0} AS s SET county=c.name_2 FROM usa_adm2 c WHERE st_intersects(c.geom, s.geom);")
for q in query:
    conn.execute(q.format(tname))
conn.close()
engine.dispose()
{% endhighlight %}

## NOAA Weather Events

Let's also add NOAA's weather events from this site: [Storm Events](https://www.ncdc.noaa.gov/stormevents/)

**In [112]:**

{% highlight python %}
# read csv and fix
datadir = '/odot/data/'
df = pd.read_csv(datadir+'noaa_OH_20140701-20150630_weather_events.csv')
df.columns = map(str.lower, df.columns)
df.columns = [c.replace(' ', '_').replace('.','') for c in df.columns]
df['begin_date'] = pd.to_datetime(df['begin_date'].astype(str), format='%m/%d/%Y')

# tell me what's in each df
nulls = np.count_nonzero(df.isnull())
print '{:15} {:d}'.format('Observations:', df.shape[0])
print '{:15} {:d}'.format('Features:', df.shape[1])
print '{:15} {:d}'.format('Nulls:', nulls)
if nulls > 0:
    print "\n---------------NULL's--------------------------\n"
    for c in df.columns:
        ns = df[c].isnull().sum()
        if ns > 0.5*df.shape[0]:
            print '{} {}'.format("Removing column", c)
            del(df[c])
    print "\n-----------------------------------------------\n"
print df.columns
{% endhighlight %}

    Observations:   447
    Features:       35
    Nulls:          0
    Index([u'event_id', u'cz_name_str', u'begin_location', u'begin_date', u'begin_time', u'event_type', u'magnitude', u'tor_f_scale', u'deaths_direct', u'injuries_direct', u'damage_property_num', u'damage_crops_num', u'state_abbr', u'cz_timezone', u'magnitude_type', u'episode_id', u'cz_type', u'cz_fips', u'wfo', u'injuries_indirect', u'deaths_indirect', u'source', u'flood_cause', u'tor_length', u'tor_width', u'begin_range', u'begin_azimuth', u'end_range', u'end_azimuth', u'end_location', u'begin_lat', u'begin_lon', u'end_lat', u'end_lon', u'absolute_rownumber'], dtype='object')


**In [113]:**

{% highlight python %}
drop = ['injuries_direct', 'damage_crops_num', 'absolute_rownumber']
df = df.rename(columns={'begin_date': 'date', 'cz_name_str': 'county'})
for d in drop:
        df = df.drop(d, 1)
{% endhighlight %}

**In [114]:**

{% highlight python %}
df.head()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>event_id</th>
      <th>county</th>
      <th>begin_location</th>
      <th>date</th>
      <th>begin_time</th>
      <th>event_type</th>
      <th>magnitude</th>
      <th>tor_f_scale</th>
      <th>deaths_direct</th>
      <th>damage_property_num</th>
      <th>...</th>
      <th>tor_width</th>
      <th>begin_range</th>
      <th>begin_azimuth</th>
      <th>end_range</th>
      <th>end_azimuth</th>
      <th>end_location</th>
      <th>begin_lat</th>
      <th>begin_lon</th>
      <th>end_lat</th>
      <th>end_lon</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td> 547190</td>
      <td>  FAYETTE</td>
      <td>  </td>
      <td>2014-11-16</td>
      <td> 2100</td>
      <td>   Winter Storm</td>
      <td>  </td>
      <td>  </td>
      <td> 0</td>
      <td> 0</td>
      <td>...</td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
    </tr>
    <tr>
      <th>1</th>
      <td> 547390</td>
      <td>   SCIOTO</td>
      <td>  </td>
      <td>2014-11-16</td>
      <td> 2100</td>
      <td> Winter Weather</td>
      <td>  </td>
      <td>  </td>
      <td> 0</td>
      <td> 0</td>
      <td>...</td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
    </tr>
    <tr>
      <th>2</th>
      <td> 547389</td>
      <td>    ADAMS</td>
      <td>  </td>
      <td>2014-11-16</td>
      <td> 2100</td>
      <td> Winter Weather</td>
      <td>  </td>
      <td>  </td>
      <td> 0</td>
      <td> 0</td>
      <td>...</td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
    </tr>
    <tr>
      <th>3</th>
      <td> 547384</td>
      <td>  HOCKING</td>
      <td>  </td>
      <td>2014-11-16</td>
      <td> 2100</td>
      <td> Winter Weather</td>
      <td>  </td>
      <td>  </td>
      <td> 0</td>
      <td> 0</td>
      <td>...</td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
    </tr>
    <tr>
      <th>4</th>
      <td> 547200</td>
      <td> HIGHLAND</td>
      <td>  </td>
      <td>2014-11-16</td>
      <td> 2100</td>
      <td>   Winter Storm</td>
      <td>  </td>
      <td>  </td>
      <td> 0</td>
      <td> 0</td>
      <td>...</td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
      <td>  </td>
    </tr>
  </tbody>
</table>
<p>5 rows × 32 columns</p>
</div>



Looks clean enough for our purposes, drop into PSQL too.

**In [115]:**

{% highlight python %}
# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()

# put into table
df.to_sql(tname, engine, if_exists='replace', index=False)

# give table geo capabilities
query = "UPDATE {0} AS a SET county=c.name_2 FROM usa_adm2 c WHERE c.name_1='Ohio' and lower(c.name_2) = lower(a.county);"
conn.execute(query.format('noaa_weather_events'))
conn.close()
engine.dispose()
{% endhighlight %}

## NOAA Weather Data Aggregation

How about a roll-up of snowfall per county per day? I have weather per hour per day per sample site. So what I want is to find the average precip over a region per hour, then sum those averages in a day per county...

{% highlight sql %}
drop table noaa_weather_agg;
create table noaa_weather_agg as
select sub.date as date,
       sub.county as county,
       round(sum(sub.avg_sacc)::numeric,2) as avg_snow,
       round(sum(sub.max_sacc)::numeric,2) as max_snow,
       round(sum(sub.min_sacc)::numeric,2) as min_snow,
       round(sum(sub.avg_pacc)::numeric,2) as avg_rain,
       round(sum(sub.max_pacc)::numeric,2) as max_rain,
       round(sum(sub.min_pacc)::numeric,2) as min_rain
from (
      select date,
             county,
             avg(snow) as avg_sacc,
             max(snow) as max_sacc,
             min(snow) as min_sacc,
             avg(prcp)/10.*0.0393701 as avg_pacc,
             max(prcp)/10.*0.0393701 as max_pacc,
             min(prcp)/10.*0.0393701 as min_pacc
      from noaa_weather
      where snow > -1 and prcp > -1
      group by 1,2
      order by 1,2 asc) as sub
group by 1,2
order by 1,2 asc;
{% endhighlight %}

Create a table of the distinct stations:
{% highlight sql %}
drop table noaa_sampling;
create table noaa_sampling as
    select station, lat, lng, county, geom
    from noaa_weather
    where station in (select distinct(station) from noaa_weather)
    group by 1,2,3,4,5
    order by 1 asc;
CREATE INDEX noaa_sampling_geom_idx ON public.noaa_sampling USING gist(geom);
{% endhighlight %}

Turns out that a couple counties have no reporting stations in the NOAA data. They are Wyandot, Belmont, and Morgan. Where do these counties rank in the total spend?
{% highlight sql %}
select county, rank 
from (select county,
             rank() over(order by sum(direct_cost) desc)
      from material group by 1) as sub
where county in ('Wyandot','Belmont','Morgan');

 county  | rank 
---------+------
 Belmont |   16
 Morgan  |   63
 Wyandot |   71
{% endhighlight %}

**In [7]:**

{% highlight python %}
# Let's add the snowfall totals to the summary table:
# open the engine to the psql db
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
conn = engine.connect()
query = '''
        ALTER TABLE summary DROP COLUMN IF EXISTS {0};
        ALTER TABLE summary ADD COLUMN {0} numeric DEFAULT 0.0;
        UPDATE summary AS s SET {0}=sub2.{0} FROM (
            select round(sum(sub1.avg_accum)::numeric,4) as avg_snow,
                   round(sum(sub1.max_accum)::numeric,4) as max_snow,
                   round(sum(sub1.min_accum)::numeric,4) as min_snow,
                   sub1.county
            from (
                  select avg(snow) as avg_accum,
                         max(snow) as max_accum,
                         min(snow) as min_accum,
                         date,
                         county
                  from noaa_weather
                  where snow >= 0
                  group by 4,5) as sub1
            group by 4) as sub2
        WHERE sub2.county=s.county;'''

fields = ['avg_snow', 'max_snow', 'min_snow']
for f in fields:
    conn.execute(query.format(f))
    
# close the engine
conn.close()
engine.dispose()
{% endhighlight %}

## Pre- / Treatment Activities vs. FIO Weather

I want to see the normalized pre-treatment, treatment materials vs snowfall in case there are any obvious trends.

**In [8]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')

# create empty dataframe for totals
rg = pd.date_range(dstr, dend).tolist()
x0 = pd.Series(rg, name='date')
y0 = pd.Series([0] * len(x0), name='amount')
dft = pd.concat([x0, y0], axis=1)

# get top ten materials
query = '''select work_date,
                  material_master_code_desc,
                  sum(amount)
           from material
           where work_date between '{0}' and '{1}'
           and material_master_code_desc in (select sub.a
                                             from (select material_master_code_desc a,
                                                          sum(amount)
                                                   from material
                                                   group by 1
                                                   order by 2 desc
                                                   limit 10
                                                   ) as sub
                                            )
           and act_name = '{2}'
           group by 1,2
           order by 1 asc;'''

#####################
### Pre-Treatment ###
#####################

df = pd.read_sql_query(query.format(dstr, dend, 'pre-treatment'), engine)

# add values for each material
mpl_fig = plt.figure()
ax1 = mpl_fig.add_subplot(211)
ax1.set_ylabel('Norm. Matl. Use')
materials = sorted(df['material_master_code_desc'].unique())
for m in materials:
    tx = df.loc[df['material_master_code_desc'] == m]['work_date'].tolist()
    ty = df.loc[df['material_master_code_desc'] == m]['sum'].tolist()
    for i, d in enumerate(tx):
        dft.loc[dft.date == d, 'amount'] += ty[i]

# plot totals
x = dft['date'].tolist()
y = dft['amount'].tolist()
ynorm = y/max(y)
ax1.plot(x, ynorm, label='Pre-treatment')

#################
### Treatment ###
#################

df = pd.read_sql_query(query.format(dstr, dend, 'snow and ice'), engine)

# add values for each material
materials = sorted(df['material_master_code_desc'].unique())
dft = pd.concat([x0, y0], axis=1)
for m in materials:
    tx = df.loc[df['material_master_code_desc'] == m]['work_date'].tolist()
    ty = df.loc[df['material_master_code_desc'] == m]['sum'].tolist()
    for i, d in enumerate(tx):
        dft.loc[dft.date == d, 'amount'] += ty[i]

# plot totals
x = dft['date'].tolist()
y = dft['amount'].tolist()
ynorm = y/max(y)
ax1.plot(x, ynorm, label='Treatment')

###########
### FIO ###
###########

# reset totals and run weather
df = pd.read_sql_query('''select *
                          from fio_weather_agg
                          order by date asc;
                       ''', engine)

# add values for each county
ax2 = mpl_fig.add_subplot(212)
ax2.set_xlabel('Date')
ax2.set_ylabel('Snowfall [in.]')
counties = sorted(df['county'].unique())
dft = pd.concat([x0, y0], axis=1)
for c in counties:
    tx = df.loc[df['county'] == c]['date'].tolist()
    ty = df.loc[df['county'] == c]['tot_snow'].tolist()
    for i, d in enumerate(tx):
        dft.loc[dft.date == d, 'amount'] += ty[i]

# plot totals
x = dft['date'].tolist()
y = dft['amount'].tolist()
ynorm = y/max(y)
ax2.plot(x, y, label='FIO Forecasted')

############
### NOAA ###
############

# reset totals and run weather
df = pd.read_sql_query('''select *
                          from noaa_weather_agg
                          order by date asc;
                       ''', engine)

# add values for each county
counties = sorted(df['county'].unique())
dft = pd.concat([x0, y0], axis=1)
for c in counties:
    tx = df.loc[df['county'] == c]['date'].tolist()
    ty = df.loc[df['county'] == c]['avg_snow'].tolist()
    for i, d in enumerate(tx):
        dft.loc[dft.date == d, 'amount'] += ty[i]

# plot totals
x = dft['date'].tolist()
y = dft['amount'].tolist()
ynorm = y/max(y)
ax2.plot(x, y, label='NOAA Recorded')

###################
### NOAA Events ###
###################

# noaa weather events
events = pd.read_sql_query('select * from noaa_weather_events', engine)
events = events[(events['date'] >= dstr) & (events['date'] <= dend)]
types = ['Heavy Snow', 'Winter Storm']
colors = ['red', 'black']
for i, t in enumerate(types):
    a = events[events['event_type'] == t]
    a = sorted(list(set(a['date'].tolist())))
    c = colors[i]
    for b in a:
        ax2.plot((b, b), (0, 300), color=c, linestyle='--', label=t)

# close sql
engine.dispose()
        
# display interactively in plotly
py.iplot_mpl(mpl_fig)#, strip_style=True)
{% endhighlight %}




<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://plot.ly/~kcavagnolo/316.embed" height="525px" width="100%"></iframe>


Pre-treatment usage statewide appears to be high early in the snow/ice
season and decays thereafter. Could sum by week and county and fit an
envelope, but we already know this to be true: as pre-treatment and
treatment materials accumulate on roadways, the need for more material
declines (see any major roadway snow/ice meltpoint study). Treatment
responds almost 1:1 for snow events. I suspect the variation in
magnitude is related to the quality of the FIO data.

## FIO vs. NOAA

I want to see a comparison of the forecasted and actual snowfalls

**In [9]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')

# compare forecasted and observed snowfall
# these will be skewed bc not all dates appear in the join
# and bc NaN's cannot be summed
query = '''select f.county as county,
                  sum(f.tot_snow) as forecasted,
                  sum(n.avg_snow) as observed
           from fio_weather_agg f
           join noaa_weather_agg n
               on n.date = f.date
               and n.county=f.county
           group by 1;'''
df = pd.read_sql_query(query, engine)
engine.dispose()
{% endhighlight %}

**In [10]:**

{% highlight python %}
mpl_fig = plt.figure()
sns.regplot(x='observed', y='forecasted',
            data=df,
            scatter=True,
            #scatter_kws={"c": df['observed']/df['forecasted'],
            #             "cmap": plt.cm.jet,
            #             "label": df['county']}
           )
plt.title('Snowfall Per County')
plt.xlabel('Recorded [in]')
plt.ylabel('Forecasted [in]')
x = range(0,130)
plt.plot(x, x, label='Equality')
py.iplot_mpl(mpl_fig)#, strip_style=True)
{% endhighlight %}




<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://plot.ly/~kcavagnolo/318.embed" height="525px" width="100%"></iframe>



Looks like, on average, that the measured totals are higher than
forecasted, and when the forecast is wrong, it's wrong by a lot
(>20%). So these are definitely distinct datasets and should be
treated as such: FIO == forecasted and NOAA == measured.

# Weather Clustering

[t-Distributed Stochastic Neighbor Embedding (t-SNE)](http://lvdmaaten.github.io/tsne)
is one way to visualize high-dimensional data -- t-SNE attempts to
"cluster" data. I'm interested in knowing which weather events (snow
storms, ice storms, etc.) and counties are like each other. This will
help in determining how a county responds to a type of weather event,
or which counties respond to weather events like each
other. Geographically this seems obvious, but I don't want to assume
that.

**In [5]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')

# get top ten materials used in pre-treatment
query = 'select * from fio_weather_agg;'
df = pd.read_sql_query(query, engine)
engine.dispose()

df.columns
{% endhighlight %}




    Index([u'date', u'county', u'tot_snow', u'tot_rain', u'app_temp', u'air_temp', u'cloudcover', u'dewpoint', u'humidity', u'ozone', u'pressure', u'visibility', u'wind_bearing', u'wind_speed'], dtype='object')



**In [3]:**

{% highlight python %}
df.describe()
{% endhighlight %}




<div style="max-height:1000px;max-width:1500px;overflow:auto;">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>tot_snow</th>
      <th>tot_rain</th>
      <th>app_temp</th>
      <th>air_temp</th>
      <th>cloudcover</th>
      <th>dewpoint</th>
      <th>humidity</th>
      <th>ozone</th>
      <th>pressure</th>
      <th>visibility</th>
      <th>wind_bearing</th>
      <th>wind_speed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
      <td> 13288.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>     0.150805</td>
      <td>     0.066576</td>
      <td>    24.253868</td>
      <td>    30.956284</td>
      <td>     0.125056</td>
      <td>    22.129109</td>
      <td>     0.711296</td>
      <td>     0</td>
      <td>  1021.144002</td>
      <td>     8.338809</td>
      <td>   206.994958</td>
      <td>     8.646997</td>
    </tr>
    <tr>
      <th>std</th>
      <td>     0.679906</td>
      <td>     0.142433</td>
      <td>    15.613408</td>
      <td>    12.562222</td>
      <td>     0.087522</td>
      <td>    13.053255</td>
      <td>     0.110403</td>
      <td>     0</td>
      <td>     7.317384</td>
      <td>     1.814409</td>
      <td>    66.393102</td>
      <td>     3.481973</td>
    </tr>
    <tr>
      <th>min</th>
      <td>     0.000000</td>
      <td>     0.000000</td>
      <td>   -21.500000</td>
      <td>    -4.700000</td>
      <td>     0.000000</td>
      <td>   -17.100000</td>
      <td>     0.310000</td>
      <td>     0</td>
      <td>   987.800000</td>
      <td>     0.700000</td>
      <td>    12.000000</td>
      <td>     1.300000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>     0.000000</td>
      <td>     0.000000</td>
      <td>    14.000000</td>
      <td>    23.100000</td>
      <td>     0.050000</td>
      <td>    13.400000</td>
      <td>     0.630000</td>
      <td>     0</td>
      <td>  1016.100000</td>
      <td>     7.400000</td>
      <td>   170.000000</td>
      <td>     6.100000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>     0.000000</td>
      <td>     0.000000</td>
      <td>    25.300000</td>
      <td>    31.450000</td>
      <td>     0.110000</td>
      <td>    23.900000</td>
      <td>     0.710000</td>
      <td>     0</td>
      <td>  1021.500000</td>
      <td>     9.000000</td>
      <td>   216.000000</td>
      <td>     8.200000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>     0.000000</td>
      <td>     0.057000</td>
      <td>    34.800000</td>
      <td>    39.400000</td>
      <td>     0.180000</td>
      <td>    31.400000</td>
      <td>     0.790000</td>
      <td>     0</td>
      <td>  1026.000000</td>
      <td>     9.800000</td>
      <td>   256.000000</td>
      <td>    10.700000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>    23.620000</td>
      <td>     1.294000</td>
      <td>    62.100000</td>
      <td>    62.100000</td>
      <td>     0.380000</td>
      <td>    51.900000</td>
      <td>     0.970000</td>
      <td>     0</td>
      <td>  1041.900000</td>
      <td>    10.000000</td>
      <td>   348.000000</td>
      <td>    27.500000</td>
    </tr>
  </tbody>
</table>
</div>



**In [56]:**

{% highlight python %}
data = df.drop('ozone', 1).drop('air_temp', 1)
{% endhighlight %}

**In [52]:**

{% highlight python %}
nada = 1 * ((data['tot_snow'] == 0) & (data['tot_rain'] == 0))
snow = 1 * ((data['tot_snow'] > 0) & (data['tot_rain'] == 0))
rain = 1 * ((data['tot_snow'] == 0) & (data['tot_rain'] > 0))
mixd = 1 * ((data['tot_snow'] > 0) & (data['tot_rain'] > 0))

data['snowd'] = snow
data['noped'] = nada
data['raind'] = rain
data['mixed'] = mixd
{% endhighlight %}

**In [57]:**

{% highlight python %}
g = sns.pairplot(data)
{% endhighlight %}


![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_78_0.png)


**In [58]:**

{% highlight python %}
# Compute the correlation matrix
corr = data.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.5,
            linewidths=.5, cbar_kws={"shrink": .5}, ax=ax)
{% endhighlight %}




![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_79_1.png)


**In [63]:**

{% highlight python %}
# setup data for t-sne
df = data#[(data['tot_snow'] > 0.) & (data['tot_rain'] < 0.005)]
y_data = df['date']
y_data = [time.mktime(x.timetuple()) for x in y_data]
y_data = np.asarray(y_data)

x_data = df.drop('county', 1).drop('date', 1)
x_data = x_data.as_matrix()
x_data = np.asarray(x_data).astype('float64')
x_data = x_data.reshape((x_data.shape[0], -1))

# perform t-SNE embedding
from tsne import bh_sne
vis_data = bh_sne(x_data)

# plot the result
mpl_fig = plt.figure()
vis_x = vis_data[:, 0]
vis_y = vis_data[:, 1]
sns.regplot(vis_x, vis_y,
           fit_reg=False,
           scatter=True,
           #scatter_kws={"c": y_data,
           #             "cmap": plt.cm.jet,
           #             "label": y_data}
           )
plt.title('t-SNE of FIO Weather')
plt.xlabel('P1')
plt.ylabel('P2')
py.iplot_mpl(mpl_fig, strip_style=True)
{% endhighlight %}

# Efficiency Ranking

If we take all expensives as a function of snowfall, then cost/snowfall is a unit of efficiency. Who's the best and worst?

**In [11]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')

# get top ten materials used in pre-treatment
query = '''select m.county,
                  m.sum/s.sum as eff_ratio
           from (select county,
                        sum(direct_cost)
                 from material
                 group by 1) m
           join (select county,
                        sum(avg_snow)
                 from noaa_weather_agg
                 group by 1
                 having sum(avg_snow) > 0) s on s.county=m.county
           order by 2 asc;'''
df = pd.read_sql_query(query, engine)
engine.dispose()
print 'max', df['county'].loc[df['eff_ratio'].idxmax()]
print 'min', df['county'].loc[df['eff_ratio'].idxmin()]
{% endhighlight %}

    max Ashland
    min Fulton


Let's compare Fulton and Ashland.

{% highlight sql %}
select county,
       act_name,
       material_master_code_desc,
       sum(direct_cost),
       sum(amount)
from material
where county in ('Ashland','Fulton')
group by 1,2,3
having sum(direct_cost) > 5000
order by 4 desc;

 county  |   act_name    | material_master_code_desc |     sum      |   sum    
---------+---------------+---------------------------+--------------+----------
 Ashland | snow and ice  | salt                      | 1019350.4918 | 17620.24
 Fulton  | snow and ice  | salt                      |  283536.3244 |  4307.75
 Fulton  | snow and ice  | calcium chloride, liquid  |    15964.672 |    29230
 Ashland | snow and ice  | beet heet concentrate     |    15218.364 | 10775.07
 Ashland | snow and ice  | salt brine                |     7749.036 |  89759.6
 Fulton  | pre-treatment | salt brine                |     7404.009 |   104420
 Ashland | snow and ice  | concrete, class c         |    6507.7492 |       31
 Ashland | pre-treatment | salt brine                |     6365.145 |    78050
{% endhighlight %}

Ashland spends substantially more on salt. Why? Lane miles, maybe?
{% highlight sql %}
select county,
       route_prio,
       sum(segment_le)
from snow_ice_priority_routes
where county in ('FULTON','ASHLAND')
group by 1,2
order by 2 asc;

 county  | route_prio |  sum  
---------+------------+-------
 FULTON  |          1 | 79.97
 ASHLAND |          1 | 77.24
 FULTON  |          2 | 15.72
 ASHLAND |          2 | 89.76
 ASHLAND |          3 | 86.71
 FULTON  |          3 | 40.06
{% endhighlight %}

Ashland has substantially more lane miles. Let's adjust the totals by this.

**In [12]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')

# get top ten materials used in pre-treatment
query = '''select initcap(lower(m.county)) as county,
                  m.sum/s.sum/r.sum as eff_ratio,
                  rank() over (order by m.sum/s.sum/r.sum asc)
           from (select lower(county) as county,
                        sum(direct_cost)
                 from material
                 group by 1) m
           join (select lower(county) as county,
                        sum(avg_snow)
                 from noaa_weather_agg
                 group by 1
                 having sum(avg_snow) > 0) s on s.county=m.county
           join (select lower(county) as county,
                        sum(segment_le)
                 from snow_ice_priority_routes
                 group by 1) r on r.county=m.county
           order by 2 asc;'''
df = pd.read_sql_query(query, engine)
engine.dispose()
print 'max', df['county'].loc[df['eff_ratio'].idxmax()]
print 'min', df['county'].loc[df['eff_ratio'].idxmin()]
{% endhighlight %}

    max Auglaize
    min Adams


Let's compare Fulton and Ashland.
{% highlight sql %}
select county,
       act_name,
       material_master_code_desc,
       sum(direct_cost),
       sum(amount)
from material
where county in ('Adams','Cuyahoga')
group by 1,2,3
having sum(direct_cost) > 5000
order by 4 desc;

  county  |   act_name   | material_master_code_desc |       sum        |   sum    
----------+--------------+---------------------------+------------------+----------
 Cuyahoga | snow and ice | salt                      |          3309231 |    67699
 Adams    | snow and ice | salt                      |           330113 |     4818
 Cuyahoga | snow and ice | calcium chloride, liquid  |           101513 |   162686
 Adams    | snow and ice | calcium chloride, liquid  |            10941 |    14596

select county,
       sum(segment_le)
from snow_ice_priority_routes
where county in ('ADAMS','CUYAHOGA')
group by 1  
order by 2 asc;

  county  |  sum   
----------+--------
 CUYAHOGA | 150.34
 ADAMS    | 216.12
{% endhighlight %}

Let's add this efficiency value to the summary table:
{% highlight sql %}
ALTER TABLE summary DROP COLUMN IF EXISTS eff_ratio;
ALTER TABLE summary ADD COLUMN eff_ratio numeric DEFAULT 0.0;
UPDATE summary AS s SET eff_ratio=sub1.eff_ratio FROM (
        select county,
               total_sum/lane_miles/avg_snow as eff_ratio
        from summary
        where avg_snow > 0) as sub1
    WHERE sub1.county=s.county;
    
ALTER TABLE summary DROP COLUMN IF EXISTS eff_rank;
ALTER TABLE summary ADD COLUMN eff_rank int DEFAULT NULL;
UPDATE summary AS s SET eff_rank=sub1.rank FROM (
        select county,
               rank() over (order by eff_ratio asc)
        from summary
        where eff_ratio is not NULL) as sub1
    WHERE sub1.county=s.county;
{% endhighlight %}

# Ideas

## Employee Network

Are there interesting connections between employees? They appear on work orders together based on date and county, so let's have a look. First need a binary employee-work order matrix

**In [4]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
query = 'select work_order_num::text, uuid::text, count(*)::numeric from labor group by 1, 2 order by 1 asc;'
df = pd.read_sql_query(query, engine)
engine.dispose()
df['uuid'] = df['uuid'].str[:5]
{% endhighlight %}

**In [5]:**

{% highlight python %}
# pivot df to be agg'd by UUID showing # of WON
ndf = df.pivot_table(values='count', index=['work_order_num'], columns='uuid', aggfunc=len, fill_value=0)

# normalize pivot by row
ndf = ndf.div(ndf.sum(axis=1), axis=0)
{% endhighlight %}

**In [8]:**

{% highlight python %}
# viz graph
import networkx as nx

# build graph
G = nx.from_numpy_matrix(coocc.as_matrix())

# get positions of nodes
position = nx.spring_layout(G)
nx.set_node_attributes(G, 'pos', position)
pos = nx.get_node_attributes(G, 'pos')
{% endhighlight %}

**In [9]:**

{% highlight python %}
dmin = 1
ncenter = 0
for n in pos:
    x,y = pos[n]
    d = (x-0.5)**2 + (y-0.5)**2
    if d < dmin:
        ncenter = n
        dmin = d
        
p = nx.single_source_shortest_path_length(G, ncenter)
{% endhighlight %}

**In [10]:**

{% highlight python %}
edge_trace = Scatter(
    x = [], 
    y = [], 
    line = Line(width=0.5, color='#888'),
    hoverinfo = 'none',
    mode = 'lines')

for edge in G.edges():
    x0, y0 = G.node[edge[0]]['pos']
    x1, y1 = G.node[edge[1]]['pos']
    edge_trace['x'] += [x0, x1, None]
    edge_trace['y'] += [y0, y1, None]

node_trace = Scatter(
    x = [], 
    y = [], 
    text = [],
    mode = 'markers', 
    hoverinfo = 'text',
    marker = Marker(
        showscale=True,
        colorscale='YIGnBu',
        reversescale=True,
        color=[], 
        size=10,         
        colorbar=dict(
            thickness=15,
            title='Node Connections',
            xanchor='left',
            titleside='right'
        ),
        line=dict(width=2)))

for node in G.nodes():
    x, y = G.node[node]['pos']
    node_trace['x'].append(x)
    node_trace['y'].append(y)
{% endhighlight %}

**In [11]:**

{% highlight python %}
for node, adjacencies in enumerate(G.adjacency_list()):
    node_trace['marker']['color'].append(len(adjacencies))
    node_info = '# connections: '+str(len(adjacencies))
    node_trace['text'].append(node_info)
{% endhighlight %}

**In [None]:**

{% highlight python %}
fig = Figure(data=Data([edge_trace, node_trace]),
             layout=Layout(
                title='ODOT Labor Network Graph',
                titlefont=dict(size=16),
                showlegend=False, 
                hovermode='closest',
                margin=dict(b=20,l=5,r=5,t=40),
                xaxis=XAxis(showgrid=False, zeroline=False, showticklabels=False),
                yaxis=YAxis(showgrid=False, zeroline=False, showticklabels=False)))

py.iplot(fig, filename='networkx')
{% endhighlight %}

<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://plot.ly/~kcavagnolo/326.embed" height="525px" width="100%"></iframe>


**In [13]:**

{% highlight python %}
graphistry.bind(source='src', destination='dst', node='nodeid').plot(G)
{% endhighlight %}


# Spend Clustering

## t-SNE

**In [36]:**

{% highlight python %}
# open engine to psql
engine = create_engine('postgresql://postgres:my_pswd@localhost:5432/odot')
query = 'select * from summary;'
df = pd.read_sql_query(query, engine)
engine.dispose()

df.columns
{% endhighlight %}




    Index([u'county', u'mater_sum', u'equip_sum', u'labor_sum', u'total_sum', u'avg_snow', u'max_snow', u'min_snow', u'for_snow', u'lane_miles', u'eff_ratio', u'eff_rank'], dtype='object')



**In [37]:**

{% highlight python %}
data = df[~pd.isnull(df['avg_snow'])]
data = data.dropna()
drop = ['total_sum', 'county', 'eff_rank', 'min_snow', 'max_snow']
for d in drop:
    data = data.drop(d, 1)
data['mater_sum'] /= 1e6
data['equip_sum'] /= 1e6
data['labor_sum'] /= 1e6
{% endhighlight %}

**In [38]:**

{% highlight python %}
g = sns.pairplot(data)
{% endhighlight %}


![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_106_0.png)


**In [39]:**

{% highlight python %}
# setup data for t-sne
y_data = data['avg_snow']
x_data = data.as_matrix()
x_data = np.asarray(x_data).astype('float64')
x_data = x_data.reshape((x_data.shape[0], -1))

# perform t-SNE embedding
from tsne import bh_sne
if (len(x_data) - 1) < 90.:
    perplex = (len(x_data) - 2)/3.
vis_data = bh_sne(x_data, perplexity=perplex)

# plot the result
mpl_fig = plt.figure()
vis_x = vis_data[:, 0]
vis_y = vis_data[:, 1]
sns.regplot(vis_x, vis_y,
           fit_reg=False,
           scatter=True,
           scatter_kws={"c": y_data,
                        "cmap": plt.cm.jet,
                        "label": y_data}
           )
plt.title('t-SNE of MEL Spend')
plt.xlabel('P1')
plt.ylabel('P2')
py.iplot_mpl(mpl_fig, strip_style=True)
{% endhighlight %}




<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://plot.ly/~kcavagnolo/314.embed" height="525px" width="100%"></iframe>



## K-means w/ BIC and PCA

**In [47]:**

{% highlight python %}
def compute_bic(kmeans, X):
    centers = [kmeans.cluster_centers_]
    labels  = kmeans.labels_
    m = kmeans.n_clusters
    n = np.bincount(labels)
    N, d = X.shape
    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], 'euclidean')**2) for i in range(m)])
    const_term = 0.5 * m * np.log(N) * (d+1)
    BIC = np.sum([n[i] * np.log(n[i]) -
                  n[i] * np.log(N) -
                  ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -
                  ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term
    return(BIC)
{% endhighlight %}

**In [48]:**

{% highlight python %}
def get_pca(x_raw, df, mv):

    # todo transform to polar coordinates since k-means assumes spherical symmetry
    # some code like np.polar function on x_raw

    # impute for missing values
    imp = preprocessing.Imputer(missing_values=mv, strategy='mean', axis=0, verbose=1)
    x_imp = imp.fit_transform(x_raw)
    
    # scale data
    x_scl = preprocessing.StandardScaler().fit_transform(x_imp)
    print "Original shape:\n%i samples\n%i features\n" % (x_scl.shape[0], x_scl.shape[1])
    
    # run pca
    pca = decomposition.PCA(n_components='mle', whiten=True)
    x_pca = pca.fit(x_scl).transform(x_scl)
    print "Reduced shape:\n%i samples\n%i features\n" % (x_pca.shape[0], x_pca.shape[1])
    print "Variance ratio:\n%s\n" % str(pca.explained_variance_ratio_)

    # describe the new subspace in terms of features
    i = np.identity(df.shape[1])
    coef = pca.transform(i)
    columns = ['PC-'+str(x) for x in range(pca.n_components_)]
    df_t = pd.DataFrame(coef, columns=columns, index=df.columns)
    print df_t
    return x_pca
{% endhighlight %}

**In [53]:**

{% highlight python %}
def get_cluster(in_pca):

    # iterate over n_cluster values
    ks = range(2, 20)
    k_means = [cluster.KMeans(n_clusters = i, init="k-means++", n_jobs=-1).fit(in_pca) for i in ks]
    BIC = [-1 * compute_bic(kmeansi, pca) for kmeansi in k_means]

    # plot result
    plt.plot(ks, BIC, 'r-o')
    plt.xlabel("N-clusters")
    plt.ylabel("BIC")
    plt.show()

    # predictions
    midx = BIC.index(min(BIC))
    k_means = k_means[midx]
    n_clusters = ks[midx]
    y_pred = k_means.predict(in_pca)

    # plot the result
    a = list(color.cnames)
    b = string.ascii_lowercase
    clusters = range(n_clusters)
    colors = a[:n_clusters]
    names = b[:n_clusters]
    for c, i, n in zip(colors, clusters, names):
        plt.scatter(in_pca[y_pred == i, 0], in_pca[y_pred == i, 1], c=c, label=n)
    plt.plot(k_means.cluster_centers_[:, 0], k_means.cluster_centers_[:, 1], 'r*', label='centers', ms=18)
    plt.show()
{% endhighlight %}

**In [54]:**

{% highlight python %}
# get some data
X = x_data
Y = y_data
pca = get_pca(X, data, 0.0)
get_cluster(pca)
{% endhighlight %}

    Original shape:
    79 samples
    7 features
    
    Reduced shape:
    79 samples
    6 features
    
    Variance ratio:
    [ 0.51755176  0.2256285   0.12603012  0.10287227  0.01924474  0.00691957]
    
                    PC-0      PC-1      PC-2      PC-3      PC-4      PC-5
    mater_sum   0.269727 -0.003876  0.047340 -0.051248  0.759112  3.660839
    equip_sum   0.266609 -0.103582 -0.069215 -0.070984 -1.183243 -1.083207
    labor_sum   0.266894 -0.092459 -0.089187 -0.108525 -1.161662 -0.497056
    avg_snow    0.174016  0.328927 -0.611425  0.003639  1.401784 -1.603723
    for_snow    0.078072  0.396748  0.339202  0.915449 -0.420455 -0.055381
    lane_miles  0.075907  0.370587  0.668401 -0.667159  0.288976 -0.865143
    eff_ratio   0.137189 -0.459262  0.427635  0.293395  1.368255 -1.581065



![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_112_1.png)



![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_112_2.png)


## Modeling Spend

**In [None]:**

{% highlight python %}
data = df[~pd.isnull(df['lane_miles'])]
spend = 1 * (data['mater_sum'] + data['equip_sum'] + data['labor_sum'] >= 1e6)
lane_miles2 = np.square(data['lane_miles'])
data = data[['mater_sum', 'equip_sum', 'labor_sum', 'total_sum', 'lane_miles']]
data['lane_miles2'] = lane_miles2
data['spend'] = spend
{% endhighlight %}

**In [56]:**

{% highlight python %}
sns.jointplot("lane_miles", "total_sum", data, kind='reg');
{% endhighlight %}


![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_115_0.png)


**In [57]:**

{% highlight python %}
sns.lmplot("lane_miles", "total_sum", data, col="spend");
{% endhighlight %}


![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_116_0.png)


**In [10]:**

{% highlight python %}
# Compute the correlation matrix
corr = data.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.,
            linewidths=.5, cbar_kws={"shrink": .5}, ax=ax)
{% endhighlight %}




![png]({{ site.baseurl }}/blog/notebooks/odot_files/odot_117_1.png)

All of this analysis was then crafted into a Tableau story by-hand. I
don't like Tableau as a distribution method for analysis unless it's
one-off and does not need to be repeated many times.

<script type='text/javascript' src='https://public.tableau.com/javascripts/api/viz_v1.js'></script><div class='tableauPlaceholder' style='width: 1220px; height: 1033px;'><noscript><a href='#'><img alt='Material, Equipment, Labor (MEL) Summary ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;da&#47;dash_16&#47;Story1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz' width='1220' height='1033' style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='site_root' value='' /><param name='name' value='dash_16&#47;Story1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;da&#47;dash_16&#47;Story1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='showVizHome' value='no' /><param name='showTabs' value='y' /><param name='bootstrapWhenNotified' value='true' /></object></div>
