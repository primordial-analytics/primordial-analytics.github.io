---
layout: post
title: "Geolocating Billions of GPS Points using Hive and Spark"
author: Ken Cavagnolo
category : ipynotebooks
tags: [python, notebook, jupyter, spark, hive, postgresql, postgis]
comments: true
---

For  a  client  project,  we   had  to  analyze  billions  of  lat/lon
coordinates collected via GPS devices. Below  are my notes on how this
was approached. The code assumes an AWS PSQL+PostGIS installation that
is port forwarded locally and a locally mounted AWS drive.

# Postgresql + PostGIS

* Raw data is in the form of [lat, lon] tuples
* Need to get into [country, region, dma, city] tuples
* This can then be coded to city... but how? Need some named entity to associate with, e.g. [TIGER geocoding](https://www.census.gov/geo/maps-data/data/tiger.html)
* TIGER geocoder only works in PSQL and not Hive which is where everything lives in AWS
* Conversion does not look feasible since we have inoperable technologies

Just for fun, let's try measuring the performance of using
Postgresql + PostGIS to create a lightweight single dataset of
country + region + dma + city. Found some incredibly useful datasets:

* [Worldwide region shapes](http://www.gadm.org/)
* [Worldwide city lat/lon](http://download.geonames.org/export/dump/)
* [Worldwide postal codes](http://download.geonames.org/export/zip/)
* [US DMA's](http://geocommons.com/overlays/463399)

Steps to complete:

* Load this data into PSQL w/ PostGIS extended tables
* Could use magic <code><-></code> operator in PGIS to associate a GPS point with the nearest city
* The below is a poorman's "geocoding" method

Load the GADM dataset into PSQL:
{% highlight tcsh %}
unzip gadm.shp.zip
dropdb gadm
createdb gadm
psql -d gadm -c "CREATE EXTENSION postgis;"
shp2pgsql -I -s 4326 -W "LATIN1" gadm.shp gadm > gadm.sql
psql -d gadm -q -f gadm.sql
{% endhighlight %}

Actually doesn't take long to run (12.5 min). Add Geonames into
db's and then clean the data some:

Creation didn't fail, so let's test it:
{% highlight sql %}
select gid, name_0, name_1, name_2 from gadm where name_0 = 'United States' and name_1 = 'Georgia';
  gid   |    name_0     | name_1  |  name_2  
--------+---------------+---------+----------
 205793 | United States | Georgia | Morgan
 205794 | United States | Georgia | Carroll
 205797 | United States | Georgia | Douglas

SELECT name, latitude, longitude FROM cities ORDER BY geom <-> st_setsrid(st_makepoint(-90,40),4326) LIMIT 5;
     name      | latitude | longitude 
---------------+----------+-----------
 Springfield   | 39.80172 | -89.64371
 JeffersonCity |  38.5767 | -92.17352
 Madison       | 43.07305 | -89.40123
 Indianapolis  | 39.76838 | -86.15804
 DesMoines     | 41.60054 | -93.60911
{% endhighlight %}

What about running NN on cities and postal codes?
{% highlight sql %}
SELECT
  DISTINCT ON(g1.geonameid) g1.geonameid AS gref_gid,
  g1.name,
  g1.country_code,
  g1.admin1_code,
  g1.admin2_code,
  g2.postal_code
FROM cities AS g1, postal AS g2
WHERE ST_DWithin(g1.geom, g2.geom, 1000)
ORDER BY g1.geonameid, ST_Distance(g1.geom,g2.geom)
LIMIT 10;
{% endhighlight %}

Well, that was painfully slow. Don't forget to build spatial indices
for all tables and vacuum analyze:
{% highlight sql %}
CREATE INDEX [tablename]_geom_gist ON [tablename] USING GIST ( [geometrycolumn] );
VACUUM ANALYZE [tablename] ( [geometrycolumn] );
{% endhighlight %}

{% highlight tcsh %}
# pre index
time psql -q -d gadm -c "SELECT c.name, c.latitude, c.longitude, c.country_code, g.name_0, g.name_1, g.name_2 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom) and c.country_code = 'US';"
> 7:39.22

# post index
time psql -q -d gadm -c "SELECT c.name, c.latitude, c.longitude, c.country_code, g.name_0, g.name_1, g.name_2 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom) and c.country_code = 'US';"
> 0:57.58
{% endhighlight %}

No shit, a speed-up of ~100x. How long does it take to geolocate all points in the cities db?
{% highlight tcsh %}
time psql -q -d gadm -c "SELECT c.name, c.latitude, c.longitude, c.country_code, g.name_0, g.name_1, g.name_2 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom);"
> 1:02:14.46
{% endhighlight %}

Approximately an hour to complete on our micro-instance, that's not terrible really. Let's build a new table with all this info:
{% highlight tcsh %}
psql -q -d gadm -c "CREATE TABLE geocode as SELECT c.geonameid, c.name, c.latitude, c.longitude, c.feature_class, c.feature_code, c.country_code, c.admin1_code, c.admin2_code, c.admin3_code, c.admin4_code, c.population, c.elevation, c.dem, c.timezone, c.mod_date, g.gid, g.objectid, g.id_0, g.iso, g.name_0, g.id_1, g.name_1, g.nl_name_1, g.hasc_1, g.cc_1, g.type_1, g.engtype_1, g.validfr_1, g.validto_1, g.remarks_1, g.id_2, g.name_2, g.nl_name_2, g.hasc_2, g.cc_2, g.type_2, g.engtype_2, g.validfr_2, g.validto_2, g.remarks_2, g.id_3, g.name_3, g.nl_name_3, g.hasc_3, g.type_3, g.engtype_3, g.validfr_3, g.validto_3, g.remarks_3, g.id_4, g.name_4, g.type4, g.engtype4, g.type_4, g.engtype_4, g.validfr_4, g.validto_4, g.remarks_4, g.id_5, g.name_5, g.type_5, g.engtype_5 FROM cities c, gadm g WHERE ST_Within(c.geom, g.geom);"
{% endhighlight %}

Dump table to csv's for posterity:
{% highlight tcsh %}
psql -q -d gadm -c "COPY (select geonameid, latitude, longitude, name as city, name_0 as country, name_1 as region, name_2 as admin_reg from geocode) TO '/tmp/geocode_all.csv' DELIMITER ',' CSV HEADER;"

psql -q -d gadm -c "COPY (select geonameid, latitude, longitude, name as city, name_0 as country, name_1 as region, name_2 as admin_reg from geocode where population > 10000) TO '/tmp/geocode_10k.csv' DELIMITER ',' CSV HEADER;"

psql -q -d gadm -c "COPY (select geonameid, latitude, longitude, name as city, name_0 as country, name_1 as region, name_2 as admin_reg from geocode where population > 100000) TO '/tmp/geocode_100k.csv' DELIMITER ',' CSV HEADER;"
{% endhighlight %}

Make a subsample of the full csv if needed for quick analysis:
{% highlight tcsh %}
head -n10000 geocode.csv > geo_test.csv
{% endhighlight %}

The above produces a master table (geocode) that has every city in the
geonames data set assigned to a country, region, admin (which is a
redundant effort since this is in the geonames dataset already). But,
this is a performance test. Tables are 10's of Gb, but dropping g.geom
col reduces to a few Gb. Total of ~4M entires in csv, which is
~1Gb. But, how to take the GPS activities in AWS and assign each to a
single record in this table? Can't do it in PSQL, why?

* Client hates PSQL and has invested heavily in Hadoop infra
* PSQL build was not standard in AWS anyways
* Would be impossibly slow for 90M+ unique acitivies

# Spark

Back of the envelope calc suggests location assignment will be the
bottleneck in client's system. So, need to get the master dataset
creation into Hive and then run big calculation in Spark.

## Option A: Broadcast Geo Tables

* Read GPS activity data out of Hive table into RDD
* Read geocode table into RDD
* Broadcast geocode data to workers
* Build function to calculate NN:
    * filter GADM points to be within 0.1 degree (1 deg ~ 110 km, so 10 km) of activity avg lat/lon
    * calc haversine distance between all remaining points
    * select GADM point with min distance
    * return geonameid of this point and all its attributes

First need some fake data to test:
{% highlight python %}
from faker import Factory
faker = Factory.create()
f = open('/mnt/aws/geoloc/data/gps_fake.csv','w')
for i in range(0,50000):
    f.write(("%s,%f,%f\n") % (faker.city(), faker.latitude(), faker.longitude()))
f.close()

!hdfs dfs -mkdir /mnt/aws/geoloc/data/test
!hdfs dfs -copyFromLocal /mnt/aws/geoloc/data/* /mnt/aws/geoloc/data/test
{% endhighlight %}

Get Spark context loaded:
{% highlight python %}
from pyspark import SparkContext, SparkConf
from pyspark.sql import HiveContext, SQLContext
from datetime import datetime
from operator import itemgetter
from math import radians, cos, sin, asin, sqrt

# load jars
jar_path = '/usr/local/hadoop/gis/gis-tools-hadoop-2.0.jar' 
spark_config = SparkConf().setMaster('local').setAppName('geoloc').set("spark.jars", jar_path) 
sc = SparkContext(conf=spark_config) 
hc = HiveContext(sc)

# get geonames test data
geo_file = sc.textFile("/mnt/aws/geoloc/data/geo_test.csv", minPartitions=16)
geo_rdd = (geo_file
           .map(lambda x: x.split(","))
           .cache())

# drop the rdd header
header = geo_rdd.first()
geo_rdd = geo_rdd.filter(lambda line: line != header)

# convert rdd to flat list and broadcast
geos = sc.broadcast(geo_rdd.collect())

# check file size
print("%i cities in geo file" % (geo_rdd.count()))
{% endhighlight %}

    30721 cities in geo file

Run some processing on the fake GPS data to benchmark:

{% highlight python %}
# read fake gps data into rdd
gps_file = sc.textFile("/mnt/aws/geoloc/data/gps_fake.csv", minPartitions=16)
gps_rdd = (gps_file
           .map(lambda x: x.split(","))
           .cache())
print("%i GPS points in geo file" % (gps_rdd.count()))
{% endhighlight %}

    50000 GPS points in geo file

{% highlight python %}
# testing speed of the filtering
lat1, lon1 = 28.215, 40.110
nns = [x for x in geos.value if (abs(lon2-lon1) < 0.5 and abs(lat2-lat1) < 0.5)]
strtime = datetime.utcnow()
for g in nns:
    lon2, lat2 = float(g[2]), float(g[1])
    lon2, lat2 = map(radians, [lon2, lat2])
    dlon = lon2 - lon1 
    dlat = lat2 - lat1 
    a = sin(dlat/2.)**2. + cos(lat1) * cos(lat2) * sin(dlon/2.)**2.
    c = 2. * asin(sqrt(a)) 
    km = 6367. * c
interval = str(datetime.utcnow()-strtime)
print("Runtime: %s" % (interval))
{% endhighlight %}

{% highlight python %}
# setup the haversine equation using broadcast
def haversine(pnt):
    lon1, lat1 = float(pnt[2]), float(pnt[1])
    nns = [x for x in geos.value if (abs(float(x[2])-lon1) < 0.1 and abs(float(x[1])-lat1) < 0.1)]
    lon1, lat1 = map(radians, [lon1, lat1])
    dists = []
    add = dists.append
    for g in nns:
        lon2, lat2 = float(g[2]), float(g[1])
        lon2, lat2 = map(radians, [lon2, lat2])
        dlon = lon2 - lon1 
        dlat = lat2 - lat1 
        a = sin(dlat/2.)**2. + cos(lat1) * cos(lat2) * sin(dlon/2.)**2.
        c = 2. * asin(sqrt(a)) 
        km = 6367. * c
        add((g[0], km))
    if len(dists) >= 1:
        return min(dists,key=itemgetter(1))
    else:
        return ('None', -1.0)
{% endhighlight %}

{% highlight python %}
# run the calculation
strtime = datetime.utcnow()
nn = gps_rdd.map(lambda x: haversine(x)).collect()
interval = str(datetime.utcnow()-strtime)
print("Runtime: %s" % (interval))
{% endhighlight %}

    Runtime: 0:07:47.142955

That sucks.

{% highlight python %}
print gps_rdd.take(2)
print nn[1]

list3 = [e for e in geos.value if e[0] in nn[1]]
print list3
{% endhighlight %}

    [[u'Trantowborough', u'-49.030777', u'-31.282075'], [u'West Elnashire', u'28.394176', u'-163.416677']]
    ('None', -1.0)
    []


The below code doesn't work. Initial attempt was to map gps_rdd to map
of haversine, but that's a nested rdd transaction which is not
supported in Spark: *"No, it is not possible, because the items of an
RDD must be serializable and a RDD is not serializable. And this makes
sense, otherwise you might transfer over the network a whole RDD which
is a problem if it contains a lot of data. And if it does not contain
a lot of data, you might and you should use an array or something like
it."*


{% highlight python %}
# define a nearest neighbor function to return just the city geonameid
def find_nn(pnt, geo_rdd):
    nn = (geo_rdd
          .map(lambda x: (int(x[0]), haversine((float(x[2]), float(x[1])), (pnt[1], pnt[0]))))
          .takeOrdered(1, lambda s: s[1]))
    return nn[0][0]

alat = 33.756011
alon = -84.389018
find_nn((alat,alon), geo_rdd)
nn_rdd = gps_rdd.map(lambda x: find_nn((x[1],x[2]), geo_rdd))
nn_rdd.take(5)
{% endhighlight %}


{% highlight python %}
# try to determine the RDD types automatically
import pandas as pd
df = pd.read_csv("/tmp/test.csv", sep=',', nrows=1)
names = df.columns
types = []
for i in range(len(names)):
    tp = names[i] + " "
    if df.dtypes[i] == "O":
        tp += "STRING"
    elif df.dtypes[i] == "int64":
        tp += "INT"
    else:
        tp += "DOUBLE"
    types.append(tp)
print types
{% endhighlight %}


## Option B: Rtree's

Alternate method using Rtree's that I did not investigate much. It's
ostensibly fast, but I have scaling concerns, specifically one cannot
broadcast an rtree -- it has to be built monolitically by one
process. **Interesting side project.**

{% highlight python %}
from rtree import index

def build_tree(record):
    rtree.insert(record[0], (record[1], record[2]))

def nn_lookup(rtree, lat, lon):
    return list(rtree.nearest(lat, lon))[0]

# build rtree from master geocode table
rtree_idx = parts.build_tree()

# read lat, lon from gps activity table
lookup(rtree_idx, lat, lon)
{% endhighlight %}

# Hive

In order to use the nearest neighbor method, need to create real GIS
lookups in Hive so Spark can ingest them quickly.

* As-is, the Shapefile and GeoJSON formats are incompatible with [ESRI's Hadoop GIS tooling](https://esri.github.io/gis-tools-for-hadoop/)
* Tooling expects un/enclosed format
* Soooo.... what about just doing it myself?
* Reference links:
    * [JSON SerDe](https://github.com/Esri/spatial-framework-for-hadoop/wiki/Hive-JSON-SerDe)
    * [JSON Utils](https://github.com/Esri/spatial-framework-for-hadoop/wiki/JSON-Utilities)
    * [ESRI geom defs](http://help.arcgis.com/en/arcgisserver/10.0/apis/rest/geometry.html)
* Testing this out on local cluster which has setup:
    * Linux 2.6.32-358.el6.x86_64 #1 SMP Fri Feb 22 00:31:26 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
    * Hive 0.14.0.2.2.0.0-2041
    * CentOS release 6.4 (Final)
* To reformat the geojson output by ogr2ogr, need to do the following:
    * Remove the leading fields:
        "type": "FeatureCollection",
        "features": [...]
    * Remove below from each feature entry:
        "type": "Feature",
    * Rename (a) to (b):
        * (a) "properties"
        * (b) "attributes"
    * Replace (a) with (b):
        * (a) "type": "Polygon", "coordinates"
        * (b) "rings"
        
## Option A: Convert using OSGEO Lib

I don't like code below since it uses the osgeo lib which is nice but
creates messy json strings instead of dicts.

{% highlight python %}
from osgeo import ogr
import json
infile = r'/mnt/aws/geoloc/maps/tiger/test_tract.shp'
shpdata = ogr.Open(infile)
if not shpdata:
    raise IOError('Could not open ' + infile)
layer = shpdata.GetLayer()
fid = 0
feat = layer.GetFeature(fid)
json = feat.ExportToJson() # creates a string
json = json.replace(", ",",").replace(": ",":")
geom = feat.GetGeometryRef()
{% endhighlight %}

## Option B: Convert using Python

The below code outputs a huge JSON that can be read into Hive. This is
much sexier than the above but, it relies on fiona lib to run. However
treats shapefile as its natural format of dict.

Also need the Geonames dataset in Hive to run any point-in-polygon
(pip) queries. Steps to this end are similar to those for PSQL above:

* Get data
* Remove non-ascii characters
* Remove fields we don't need or want
* Re-format to an unenclosed JSON
* Load into Hive

To load these unenclosed json's into Hive need to:

* Load jars
* Read UDF's
* Store data in HDFS

{% highlight tcsh %}
# create dir's in HDFS
hdfs dfs -mkdir geoloc
hdfs dfs -mkdir geoloc/data
hdfs dfs -mkdir geoloc/data/tiger
hdfs dfs -mkdir geoloc/data/gadm
hdfs dfs -mkdir geoloc/data/geonames
hdfs dfs -mkdir geoloc/data/dma

# copy data over into hdfs
hdfs dfs -copyFromLocal /mnt/aws/gis-tools-for-hadoop-2.0/samples/data/* geoloc/data/
hdfs dfs -copyFromLocal /mnt/aws/tl_2014_13_tract.geojson geoloc/data/tiger/
hdfs dfs -copyFromLocal /mnt/aws/geoloc/maps/gadm/gadm_hive.json geoloc/data/gadm/
hdfs dfs -copyFromLocal /mnt/aws/geoloc/maps/geonames/cities/cities_hive.json geoloc/data/geonames/
hdfs dfs -copyFromLocal /mnt/aws/geoloc/maps/dma/dma_hive.json geoloc/data/dma/
{% endhighlight %}

{% highlight sql %}
-- add the ESRI udf's and load the functions
hive
add jar gis-tools-hadoop-2.0.jar;
add jar /usr/local/hadoop/gis/gis-tools-hadoop-2.0.jar;
create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point';
create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains';
create temporary function ST_Within as 'com.esri.hadoop.hive.ST_Within';
create temporary function ST_GeomFromGeoJson as 'com.esri.hadoop.hive.ST_GeomFromGeoJson';
create temporary function ST_AsGeoJson as 'com.esri.hadoop.hive.ST_AsGeoJson';
create temporary function ST_GeometryType as 'com.esri.hadoop.hive.ST_GeometryType';
{% endhighlight %}

* Upload the reformated json produced by script
* Build the Hive table from that file
* Run some queries against the table

{% highlight sql %}
DROP TABLE gadm;
CREATE EXTERNAL TABLE IF NOT EXISTS gadm (OBJECTID int, ID_0 int, ISO string, NAME_0 string, ID_1 int, NAME_1 string, NL_NAME_1 string, HASC_1 string, ID_2 int, NAME_2 string, NL_NAME_2 string, HASC_2 string, ID_3 int, NAME_3 string, NL_NAME_3 string, HASC_3 string, ID_4 int, NAME_4 string, TYPE4 string, ENGTYPE4 string, ID_5 int, NAME_5 string, TYPE_5 string, ENGTYPE_5 string, Shape_Leng double, Shape_Area double, Geom binary)
ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde'
STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/kcavagnolo/geolocation/data/gadm';
{% endhighlight %}

{% highlight sql %}
DROP TABLE geonames;
CREATE EXTERNAL TABLE IF NOT EXISTS geonames (geonameid string, name string, fclass string, fcode string, country string, admin1 string, admin2 string, admin3 string, admin4 string, population string, Geom binary)
ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde'
STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/kcavagnolo/geolocation/data/geonames';
{% endhighlight %}

{% highlight sql %}
DROP TABLE dma;
CREATE EXTERNAL TABLE IF NOT EXISTS dma (gid int, name string, income_6 double, income_ra0 double, income_7 double, income_lo0 double, income_8 double, dma0 double, income_70 double, dma_1 string, income_60 double, income_0 double, ebitous0 double, latitude0 double, longitude0 double, descriptio string, income_30 double, income_3 double, income_20 double, income_31 double, income_120 double, income_21 double, income_5 double, incomf_4 double, income_12 double, Geom binary)
ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.JsonSerde'
STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/kcavagnolo/geolocation/data/dma';
{% endhighlight %}

Run a cuople test queries
{% highlight sql %}
select objectid, iso, name_0, ST_GeometryType(Geom) from gadm limit 5;
select geonameid, name, ST_AsGeoJson(Geom) from geonames limit 5;
select gid, dma_1, ST_AsGeoJson(Geom) from dma limit 5;
select name_0, name_1, name_2, name_3, name_4, name_5, ST_AsGeoJson(Geom) from gadm limit 1;
SELECT name_0, name_1, name_2, name_3, name_4, name_5 FROM gadm WHERE ST_Contains(geom, ST_Point(-83.641432, 32.643191));
{% endhighlight %}

Try a binning exercise (by geo)
{% highlight sql %}
SELECT g.name_0, g.name_1, g.name_2, g.name_3, g.name_4, g.name_5, count(*) cnt FROM gadm g
JOIN geonames e
WHERE ST_Contains(g.geom, ST_Point(e.longitude, e.latitude))
GROUP BY g.name_1
ORDER BY cnt desc;
{% endhighlight %}

Try inverting the binning to get loc for each lat/lon
{% highlight sql %}
SELECT e.latitude, e.longitude, g.name_0, g.name_1, g.name_2, g.name_3 FROM geonames e
JOIN gadm g
WHERE ST_Within(ST_Point(e.longitude, e.latitude), g.geom);
{% endhighlight %}

The above works, which means running the below should return a single location for each activity of the form:

* name_0 = country, e.g. United States
* name_1 = region, e.g. Texas
* name_2 = admin, e.g. Rusk
* name_3 = dma, e.g. Midwest-Dallas
* name_4 = city, e.g. Henderson
{% highlight sql %}
SELECT *
FROM agg_gps_summary_december a
JOIN gadm g
WHERE ST_Within(ST_Point(a.avg_lon, a.avg_lat), g.geom);
{% endhighlight %}

# Location Confidence Scoring

* Let's start with the groundtruth about a user: their GPS data
* GPS tracks are detections of a user's occupancy model
* The analog is interestingly "wildlife survey data with imperfect detectability"
* This is typically modelled using MCMC
* So we've got some sweet tools to chain together
    * GIS for Hadoop to do the geolocation
    * PyMC to do the modelling of geolocation
    * PySpark to do all this quickly


{% highlight python %}
# Occupancy data - rows are sites, with replicate surveys conducted at each site
from pylab import *
from pymc import *
from numpy import *
observations = array([[0,0,0,1,1], [0,1,0,0,0], [0,1,0,0,0], [1,1,1,1,0], [0,0,1,0,0], 
                      [0,0,1,0,0], [0,0,1,0,0], [0,0,1,0,0], [0,0,1,0,0], [1,0,0,0,0], 
                      [0,0,1,1,1], [0,0,1,1,1], [1,0,0,1,1], [1,0,1,1,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], [0,0,0,0,0], 
                      [0,0,0,1,0], [0,0,0,1,0], [0,0,0,0,1], [0,0,0,0,1]])
obs_rdd = sc.parallelize(observations, 4)
{% endhighlight %}


{% highlight python %}
# map observations to flat values
k = 5
y = obs_rdd.map(lambda x: sum(x))
z_start = y.filter(lambda x: x > 0)
{% endhighlight %}


{% highlight python %}
# Prior on probability of detection
p = Beta('p', alpha=1, beta=1, value=0.99)
 
# Prior on probability of occupancy
psi = Beta('psi', alpha=1, beta=1, value=0.01)
 
#latent states for occupancy
z = Bernoulli('z', p=psi, value=z_start, plot=False)
{% endhighlight %}


{% highlight python %}
#Number of truly occupied sites in the sample (finite-sample occupancy)
@deterministic(plot=True)
def Num_occ(z=z):
   out = sum(z)
   return out
 
#unconditional probabilities of detection at each site (zero for unoccupied sites, p for occupied sites)
@deterministic(plot=False)
def pdet(z=z, p=p):
    out = z*p
    return out
 
#likelihood
def binom_mcmc(y, k, pdet):
    Y = Binomial('Y', n=k, p=pdet, value=y, observed=True)
    return Y
{% endhighlight %}


{% highlight python %}
@distributedMCMC(plot=False)

result = y.map(lambda x: binom_mcmc(x))
{% endhighlight %}

All the client proprietary work kicked in here and I cannot share
it. The steps were something like this:


* Cluster all user activities into "sites"
* Create virtual sensor network by selecting top N sites
* Assign every user detection (avg lat/lon per activity) to a site
* Model detection probability for future activities
* From probabilities, select nearest city using above methods
